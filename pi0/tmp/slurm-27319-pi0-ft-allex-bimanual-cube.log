âœ… Conda environment 'lerobot' activated.
INFO 2025-07-04 11:25:08 ts/train.py:111 {'batch_size': 24,
 'dataset': {'episodes': None,
             'image_transforms': {'enable': False,
                                  'max_num_transforms': 3,
                                  'random_order': False,
                                  'tfs': {'brightness': {'kwargs': {'brightness': [0.8,
                                                                                   1.2]},
                                                         'type': 'ColorJitter',
                                                         'weight': 1.0},
                                          'contrast': {'kwargs': {'contrast': [0.8,
                                                                               1.2]},
                                                       'type': 'ColorJitter',
                                                       'weight': 1.0},
                                          'hue': {'kwargs': {'hue': [-0.05,
                                                                     0.05]},
                                                  'type': 'ColorJitter',
                                                  'weight': 1.0},
                                          'saturation': {'kwargs': {'saturation': [0.5,
                                                                                   1.5]},
                                                         'type': 'ColorJitter',
                                                         'weight': 1.0},
                                          'sharpness': {'kwargs': {'sharpness': [0.5,
                                                                                 1.5]},
                                                        'type': 'SharpnessJitter',
                                                        'weight': 1.0}}},
             'repo_id': 'RLWRLD/allex_cube',
             'revision': None,
             'root': None,
             'use_imagenet_stats': True,
             'video_backend': 'torchcodec'},
 'env': None,
 'eval': {'batch_size': 50, 'n_episodes': 50, 'use_async_envs': False},
 'eval_freq': 20000,
 'job_name': 'pi0-ft-allex-bimanual-cube',
 'log_freq': 200,
 'num_workers': 4,
 'optimizer': {'betas': [0.9, 0.95],
               'eps': 1e-08,
               'grad_clip_norm': 10.0,
               'lr': 2.5e-05,
               'type': 'adamw',
               'weight_decay': 1e-10},
 'output_dir': 'outputs/train/2025-07-04/11-25-08_pi0-ft-allex-bimanual-cube',
 'policy': {'adapt_to_pi_aloha': False,
            'attention_implementation': 'eager',
            'chunk_size': 50,
            'device': 'cuda',
            'empty_cameras': 0,
            'ext_action_dim': 42,
            'ext_state_dim': 60,
            'freeze_vision_encoder': True,
            'input_features': {},
            'max_action_dim': 32,
            'max_state_dim': 32,
            'n_action_steps': 50,
            'n_obs_steps': 1,
            'normalization_mapping': {'ACTION': <NormalizationMode.MEAN_STD: 'MEAN_STD'>,
                                      'STATE': <NormalizationMode.MEAN_STD: 'MEAN_STD'>,
                                      'VISUAL': <NormalizationMode.IDENTITY: 'IDENTITY'>},
            'num_steps': 10,
            'optimizer_betas': [0.9, 0.95],
            'optimizer_eps': 1e-08,
            'optimizer_lr': 2.5e-05,
            'optimizer_weight_decay': 1e-10,
            'output_features': {},
            'proj_width': 1024,
            'resize_imgs_with_padding': [224, 224],
            'scheduler_decay_lr': 2.5e-06,
            'scheduler_decay_steps': 30000,
            'scheduler_warmup_steps': 1000,
            'tokenizer_max_length': 48,
            'train_expert_only': False,
            'train_state_proj': True,
            'type': 'pi0',
            'use_amp': False,
            'use_cache': True,
            'use_delta_joint_actions_aloha': False,
            'use_extended_dim': True},
 'resume': False,
 'save_checkpoint': True,
 'save_freq': 20000,
 'scheduler': {'decay_lr': 2.5e-06,
               'num_decay_steps': 30000,
               'num_warmup_steps': 1000,
               'peak_lr': 2.5e-05,
               'type': 'cosine_decay_with_warmup'},
 'seed': 1000,
 'steps': 30000,
 'use_policy_training_preset': True,
 'wandb': {'disable_artifact': True,
           'enable': True,
           'entity': None,
           'mode': None,
           'notes': None,
           'project': 'lerobot',
           'run_id': None}}
INFO 2025-07-04 11:25:13 db_utils.py:103 Track this run --> https://wandb.ai/gerald-kim-rlwrld/lerobot/runs/wbgrvtbu
INFO 2025-07-04 11:25:13 ts/train.py:127 Creating dataset
INFO 2025-07-04 11:25:15 ts/train.py:138 Creating policy
INFO 2025-07-04 11:26:12 ts/train.py:144 Creating optimizer and scheduler
INFO 2025-07-04 11:26:12 ts/train.py:156 Output dir: outputs/train/2025-07-04/11-25-08_pi0-ft-allex-bimanual-cube
INFO 2025-07-04 11:26:12 ts/train.py:159 cfg.steps=30000 (30K)
INFO 2025-07-04 11:26:12 ts/train.py:160 dataset.num_frames=13334 (13K)
INFO 2025-07-04 11:26:12 ts/train.py:161 dataset.num_episodes=2
INFO 2025-07-04 11:26:12 ts/train.py:162 num_learnable_params=3088934538 (3B)
INFO 2025-07-04 11:26:12 ts/train.py:163 num_total_params=3501377178 (4B)
INFO 2025-07-04 11:26:12 ts/train.py:202 Start offline training on a fixed dataset
Logs will be synced with wandb.
INFO 2025-07-04 11:30:26 ts/train.py:232 step:200 smpl:5K ep:1 epch:0.36 loss:1.835 grdn:2.429 lr:2.5e-06 updt_s:1.251 data_s:0.015
WARNING 2025-07-04 11:30:26 db_utils.py:141 WandB logging of key "losses_after_forward" was ignored as its type "<class 'torch.Tensor'>" is not handled by this wrapper.
WARNING 2025-07-04 11:30:26 db_utils.py:141 WandB logging of key "losses_after_in_ep_bound" was ignored as its type "<class 'torch.Tensor'>" is not handled by this wrapper.
WARNING 2025-07-04 11:30:26 db_utils.py:141 WandB logging of key "losses_after_rm_padding" was ignored as its type "<class 'torch.Tensor'>" is not handled by this wrapper.
INFO 2025-07-04 11:34:34 ts/train.py:232 step:400 smpl:10K ep:1 epch:0.72 loss:1.559 grdn:1.984 lr:7.5e-06 updt_s:1.239 data_s:0.000
WARNING 2025-07-04 11:34:34 db_utils.py:141 WandB logging of key "losses_after_forward" was ignored as its type "<class 'torch.Tensor'>" is not handled by this wrapper.
WARNING 2025-07-04 11:34:34 db_utils.py:141 WandB logging of key "losses_after_in_ep_bound" was ignored as its type "<class 'torch.Tensor'>" is not handled by this wrapper.
WARNING 2025-07-04 11:34:34 db_utils.py:141 WandB logging of key "losses_after_rm_padding" was ignored as its type "<class 'torch.Tensor'>" is not handled by this wrapper.
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
INFO 2025-07-04 11:38:46 ts/train.py:232 step:600 smpl:14K ep:2 epch:1.08 loss:1.312 grdn:2.989 lr:1.3e-05 updt_s:1.243 data_s:0.016
WARNING 2025-07-04 11:38:46 db_utils.py:141 WandB logging of key "losses_after_forward" was ignored as its type "<class 'torch.Tensor'>" is not handled by this wrapper.
WARNING 2025-07-04 11:38:46 db_utils.py:141 WandB logging of key "losses_after_in_ep_bound" was ignored as its type "<class 'torch.Tensor'>" is not handled by this wrapper.
WARNING 2025-07-04 11:38:46 db_utils.py:141 WandB logging of key "losses_after_rm_padding" was ignored as its type "<class 'torch.Tensor'>" is not handled by this wrapper.
INFO 2025-07-04 11:42:54 ts/train.py:232 step:800 smpl:19K ep:3 epch:1.44 loss:1.101 grdn:3.234 lr:1.8e-05 updt_s:1.241 data_s:0.000
WARNING 2025-07-04 11:42:54 db_utils.py:141 WandB logging of key "losses_after_forward" was ignored as its type "<class 'torch.Tensor'>" is not handled by this wrapper.
WARNING 2025-07-04 11:42:54 db_utils.py:141 WandB logging of key "losses_after_in_ep_bound" was ignored as its type "<class 'torch.Tensor'>" is not handled by this wrapper.
WARNING 2025-07-04 11:42:54 db_utils.py:141 WandB logging of key "losses_after_rm_padding" was ignored as its type "<class 'torch.Tensor'>" is not handled by this wrapper.
INFO 2025-07-04 11:47:03 ts/train.py:232 step:1K smpl:24K ep:4 epch:1.80 loss:0.900 grdn:2.810 lr:2.3e-05 updt_s:1.241 data_s:0.000
WARNING 2025-07-04 11:47:03 db_utils.py:141 WandB logging of key "losses_after_forward" was ignored as its type "<class 'torch.Tensor'>" is not handled by this wrapper.
WARNING 2025-07-04 11:47:03 db_utils.py:141 WandB logging of key "losses_after_in_ep_bound" was ignored as its type "<class 'torch.Tensor'>" is not handled by this wrapper.
WARNING 2025-07-04 11:47:03 db_utils.py:141 WandB logging of key "losses_after_rm_padding" was ignored as its type "<class 'torch.Tensor'>" is not handled by this wrapper.
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
INFO 2025-07-04 11:51:13 ts/train.py:232 step:1K smpl:29K ep:4 epch:2.16 loss:0.715 grdn:2.332 lr:2.5e-05 updt_s:1.235 data_s:0.017
WARNING 2025-07-04 11:51:13 db_utils.py:141 WandB logging of key "losses_after_forward" was ignored as its type "<class 'torch.Tensor'>" is not handled by this wrapper.
WARNING 2025-07-04 11:51:13 db_utils.py:141 WandB logging of key "losses_after_in_ep_bound" was ignored as its type "<class 'torch.Tensor'>" is not handled by this wrapper.
WARNING 2025-07-04 11:51:13 db_utils.py:141 WandB logging of key "losses_after_rm_padding" was ignored as its type "<class 'torch.Tensor'>" is not handled by this wrapper.
INFO 2025-07-04 11:55:21 ts/train.py:232 step:1K smpl:34K ep:5 epch:2.52 loss:0.572 grdn:2.106 lr:2.5e-05 updt_s:1.237 data_s:0.000
WARNING 2025-07-04 11:55:21 db_utils.py:141 WandB logging of key "losses_after_forward" was ignored as its type "<class 'torch.Tensor'>" is not handled by this wrapper.
WARNING 2025-07-04 11:55:21 db_utils.py:141 WandB logging of key "losses_after_in_ep_bound" was ignored as its type "<class 'torch.Tensor'>" is not handled by this wrapper.
WARNING 2025-07-04 11:55:21 db_utils.py:141 WandB logging of key "losses_after_rm_padding" was ignored as its type "<class 'torch.Tensor'>" is not handled by this wrapper.
INFO 2025-07-04 11:59:29 ts/train.py:232 step:2K smpl:38K ep:6 epch:2.88 loss:0.470 grdn:1.926 lr:2.5e-05 updt_s:1.239 data_s:0.000
WARNING 2025-07-04 11:59:29 db_utils.py:141 WandB logging of key "losses_after_forward" was ignored as its type "<class 'torch.Tensor'>" is not handled by this wrapper.
WARNING 2025-07-04 11:59:29 db_utils.py:141 WandB logging of key "losses_after_in_ep_bound" was ignored as its type "<class 'torch.Tensor'>" is not handled by this wrapper.
WARNING 2025-07-04 11:59:29 db_utils.py:141 WandB logging of key "losses_after_rm_padding" was ignored as its type "<class 'torch.Tensor'>" is not handled by this wrapper.
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
INFO 2025-07-04 12:03:40 ts/train.py:232 step:2K smpl:43K ep:6 epch:3.24 loss:0.394 grdn:1.733 lr:2.5e-05 updt_s:1.237 data_s:0.016
WARNING 2025-07-04 12:03:40 db_utils.py:141 WandB logging of key "losses_after_forward" was ignored as its type "<class 'torch.Tensor'>" is not handled by this wrapper.
WARNING 2025-07-04 12:03:40 db_utils.py:141 WandB logging of key "losses_after_in_ep_bound" was ignored as its type "<class 'torch.Tensor'>" is not handled by this wrapper.
WARNING 2025-07-04 12:03:40 db_utils.py:141 WandB logging of key "losses_after_rm_padding" was ignored as its type "<class 'torch.Tensor'>" is not handled by this wrapper.
INFO 2025-07-04 12:07:48 ts/train.py:232 step:2K smpl:48K ep:7 epch:3.60 loss:0.344 grdn:1.667 lr:2.5e-05 updt_s:1.240 data_s:0.000
WARNING 2025-07-04 12:07:48 db_utils.py:141 WandB logging of key "losses_after_forward" was ignored as its type "<class 'torch.Tensor'>" is not handled by this wrapper.
WARNING 2025-07-04 12:07:48 db_utils.py:141 WandB logging of key "losses_after_in_ep_bound" was ignored as its type "<class 'torch.Tensor'>" is not handled by this wrapper.
WARNING 2025-07-04 12:07:48 db_utils.py:141 WandB logging of key "losses_after_rm_padding" was ignored as its type "<class 'torch.Tensor'>" is not handled by this wrapper.
INFO 2025-07-04 12:11:57 ts/train.py:232 step:2K smpl:53K ep:8 epch:3.96 loss:0.302 grdn:1.570 lr:2.5e-05 updt_s:1.239 data_s:0.000
WARNING 2025-07-04 12:11:57 db_utils.py:141 WandB logging of key "losses_after_forward" was ignored as its type "<class 'torch.Tensor'>" is not handled by this wrapper.
WARNING 2025-07-04 12:11:57 db_utils.py:141 WandB logging of key "losses_after_in_ep_bound" was ignored as its type "<class 'torch.Tensor'>" is not handled by this wrapper.
WARNING 2025-07-04 12:11:57 db_utils.py:141 WandB logging of key "losses_after_rm_padding" was ignored as its type "<class 'torch.Tensor'>" is not handled by this wrapper.
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
INFO 2025-07-04 12:16:08 ts/train.py:232 step:2K smpl:58K ep:9 epch:4.32 loss:0.269 grdn:1.561 lr:2.5e-05 updt_s:1.238 data_s:0.017
WARNING 2025-07-04 12:16:08 db_utils.py:141 WandB logging of key "losses_after_forward" was ignored as its type "<class 'torch.Tensor'>" is not handled by this wrapper.
WARNING 2025-07-04 12:16:08 db_utils.py:141 WandB logging of key "losses_after_in_ep_bound" was ignored as its type "<class 'torch.Tensor'>" is not handled by this wrapper.
WARNING 2025-07-04 12:16:08 db_utils.py:141 WandB logging of key "losses_after_rm_padding" was ignored as its type "<class 'torch.Tensor'>" is not handled by this wrapper.
INFO 2025-07-04 12:20:16 ts/train.py:232 step:3K smpl:62K ep:9 epch:4.68 loss:0.244 grdn:1.401 lr:2.5e-05 updt_s:1.240 data_s:0.000
WARNING 2025-07-04 12:20:16 db_utils.py:141 WandB logging of key "losses_after_forward" was ignored as its type "<class 'torch.Tensor'>" is not handled by this wrapper.
WARNING 2025-07-04 12:20:16 db_utils.py:141 WandB logging of key "losses_after_in_ep_bound" was ignored as its type "<class 'torch.Tensor'>" is not handled by this wrapper.
WARNING 2025-07-04 12:20:16 db_utils.py:141 WandB logging of key "losses_after_rm_padding" was ignored as its type "<class 'torch.Tensor'>" is not handled by this wrapper.
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
INFO 2025-07-04 12:24:27 ts/train.py:232 step:3K smpl:67K ep:10 epch:5.04 loss:0.222 grdn:1.423 lr:2.5e-05 updt_s:1.238 data_s:0.017
WARNING 2025-07-04 12:24:27 db_utils.py:141 WandB logging of key "losses_after_forward" was ignored as its type "<class 'torch.Tensor'>" is not handled by this wrapper.
WARNING 2025-07-04 12:24:27 db_utils.py:141 WandB logging of key "losses_after_in_ep_bound" was ignored as its type "<class 'torch.Tensor'>" is not handled by this wrapper.
WARNING 2025-07-04 12:24:27 db_utils.py:141 WandB logging of key "losses_after_rm_padding" was ignored as its type "<class 'torch.Tensor'>" is not handled by this wrapper.
INFO 2025-07-04 12:28:36 ts/train.py:232 step:3K smpl:72K ep:11 epch:5.40 loss:0.202 grdn:1.345 lr:2.4e-05 updt_s:1.242 data_s:0.000
WARNING 2025-07-04 12:28:36 db_utils.py:141 WandB logging of key "losses_after_forward" was ignored as its type "<class 'torch.Tensor'>" is not handled by this wrapper.
WARNING 2025-07-04 12:28:36 db_utils.py:141 WandB logging of key "losses_after_in_ep_bound" was ignored as its type "<class 'torch.Tensor'>" is not handled by this wrapper.
WARNING 2025-07-04 12:28:36 db_utils.py:141 WandB logging of key "losses_after_rm_padding" was ignored as its type "<class 'torch.Tensor'>" is not handled by this wrapper.
INFO 2025-07-04 12:32:45 ts/train.py:232 step:3K smpl:77K ep:12 epch:5.76 loss:0.182 grdn:1.299 lr:2.4e-05 updt_s:1.242 data_s:0.000
WARNING 2025-07-04 12:32:45 db_utils.py:141 WandB logging of key "losses_after_forward" was ignored as its type "<class 'torch.Tensor'>" is not handled by this wrapper.
WARNING 2025-07-04 12:32:45 db_utils.py:141 WandB logging of key "losses_after_in_ep_bound" was ignored as its type "<class 'torch.Tensor'>" is not handled by this wrapper.
WARNING 2025-07-04 12:32:45 db_utils.py:141 WandB logging of key "losses_after_rm_padding" was ignored as its type "<class 'torch.Tensor'>" is not handled by this wrapper.
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
INFO 2025-07-04 12:36:57 ts/train.py:232 step:3K smpl:82K ep:12 epch:6.12 loss:0.165 grdn:1.212 lr:2.4e-05 updt_s:1.241 data_s:0.016
WARNING 2025-07-04 12:36:57 db_utils.py:141 WandB logging of key "losses_after_forward" was ignored as its type "<class 'torch.Tensor'>" is not handled by this wrapper.
WARNING 2025-07-04 12:36:57 db_utils.py:141 WandB logging of key "losses_after_in_ep_bound" was ignored as its type "<class 'torch.Tensor'>" is not handled by this wrapper.
WARNING 2025-07-04 12:36:57 db_utils.py:141 WandB logging of key "losses_after_rm_padding" was ignored as its type "<class 'torch.Tensor'>" is not handled by this wrapper.
INFO 2025-07-04 12:41:06 ts/train.py:232 step:4K smpl:86K ep:13 epch:6.48 loss:0.152 grdn:1.164 lr:2.4e-05 updt_s:1.242 data_s:0.000
WARNING 2025-07-04 12:41:06 db_utils.py:141 WandB logging of key "losses_after_forward" was ignored as its type "<class 'torch.Tensor'>" is not handled by this wrapper.
WARNING 2025-07-04 12:41:06 db_utils.py:141 WandB logging of key "losses_after_in_ep_bound" was ignored as its type "<class 'torch.Tensor'>" is not handled by this wrapper.
WARNING 2025-07-04 12:41:06 db_utils.py:141 WandB logging of key "losses_after_rm_padding" was ignored as its type "<class 'torch.Tensor'>" is not handled by this wrapper.
INFO 2025-07-04 12:45:14 ts/train.py:232 step:4K smpl:91K ep:14 epch:6.84 loss:0.140 grdn:1.130 lr:2.4e-05 updt_s:1.239 data_s:0.000
WARNING 2025-07-04 12:45:14 db_utils.py:141 WandB logging of key "losses_after_forward" was ignored as its type "<class 'torch.Tensor'>" is not handled by this wrapper.
WARNING 2025-07-04 12:45:14 db_utils.py:141 WandB logging of key "losses_after_in_ep_bound" was ignored as its type "<class 'torch.Tensor'>" is not handled by this wrapper.
WARNING 2025-07-04 12:45:14 db_utils.py:141 WandB logging of key "losses_after_rm_padding" was ignored as its type "<class 'torch.Tensor'>" is not handled by this wrapper.
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
INFO 2025-07-04 12:49:25 ts/train.py:232 step:4K smpl:96K ep:14 epch:7.20 loss:0.130 grdn:1.137 lr:2.4e-05 updt_s:1.236 data_s:0.017
WARNING 2025-07-04 12:49:25 db_utils.py:141 WandB logging of key "losses_after_forward" was ignored as its type "<class 'torch.Tensor'>" is not handled by this wrapper.
WARNING 2025-07-04 12:49:25 db_utils.py:141 WandB logging of key "losses_after_in_ep_bound" was ignored as its type "<class 'torch.Tensor'>" is not handled by this wrapper.
WARNING 2025-07-04 12:49:25 db_utils.py:141 WandB logging of key "losses_after_rm_padding" was ignored as its type "<class 'torch.Tensor'>" is not handled by this wrapper.
INFO 2025-07-04 12:53:33 ts/train.py:232 step:4K smpl:101K ep:15 epch:7.56 loss:0.119 grdn:1.065 lr:2.4e-05 updt_s:1.239 data_s:0.000
WARNING 2025-07-04 12:53:33 db_utils.py:141 WandB logging of key "losses_after_forward" was ignored as its type "<class 'torch.Tensor'>" is not handled by this wrapper.
WARNING 2025-07-04 12:53:33 db_utils.py:141 WandB logging of key "losses_after_in_ep_bound" was ignored as its type "<class 'torch.Tensor'>" is not handled by this wrapper.
WARNING 2025-07-04 12:53:33 db_utils.py:141 WandB logging of key "losses_after_rm_padding" was ignored as its type "<class 'torch.Tensor'>" is not handled by this wrapper.
INFO 2025-07-04 12:57:42 ts/train.py:232 step:4K smpl:106K ep:16 epch:7.92 loss:0.110 grdn:1.014 lr:2.4e-05 updt_s:1.241 data_s:0.000
WARNING 2025-07-04 12:57:42 db_utils.py:141 WandB logging of key "losses_after_forward" was ignored as its type "<class 'torch.Tensor'>" is not handled by this wrapper.
WARNING 2025-07-04 12:57:42 db_utils.py:141 WandB logging of key "losses_after_in_ep_bound" was ignored as its type "<class 'torch.Tensor'>" is not handled by this wrapper.
WARNING 2025-07-04 12:57:42 db_utils.py:141 WandB logging of key "losses_after_rm_padding" was ignored as its type "<class 'torch.Tensor'>" is not handled by this wrapper.
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
INFO 2025-07-04 13:01:53 ts/train.py:232 step:5K smpl:110K ep:17 epch:8.28 loss:0.101 grdn:1.037 lr:2.4e-05 updt_s:1.239 data_s:0.016
WARNING 2025-07-04 13:01:53 db_utils.py:141 WandB logging of key "losses_after_forward" was ignored as its type "<class 'torch.Tensor'>" is not handled by this wrapper.
WARNING 2025-07-04 13:01:53 db_utils.py:141 WandB logging of key "losses_after_in_ep_bound" was ignored as its type "<class 'torch.Tensor'>" is not handled by this wrapper.
WARNING 2025-07-04 13:01:53 db_utils.py:141 WandB logging of key "losses_after_rm_padding" was ignored as its type "<class 'torch.Tensor'>" is not handled by this wrapper.
INFO 2025-07-04 13:06:01 ts/train.py:232 step:5K smpl:115K ep:17 epch:8.64 loss:0.095 grdn:0.953 lr:2.4e-05 updt_s:1.241 data_s:0.000
WARNING 2025-07-04 13:06:01 db_utils.py:141 WandB logging of key "losses_after_forward" was ignored as its type "<class 'torch.Tensor'>" is not handled by this wrapper.
WARNING 2025-07-04 13:06:01 db_utils.py:141 WandB logging of key "losses_after_in_ep_bound" was ignored as its type "<class 'torch.Tensor'>" is not handled by this wrapper.
WARNING 2025-07-04 13:06:01 db_utils.py:141 WandB logging of key "losses_after_rm_padding" was ignored as its type "<class 'torch.Tensor'>" is not handled by this wrapper.
INFO 2025-07-04 13:10:09 ts/train.py:232 step:5K smpl:120K ep:18 epch:9.00 loss:0.088 grdn:0.968 lr:2.4e-05 updt_s:1.239 data_s:0.000
WARNING 2025-07-04 13:10:09 db_utils.py:141 WandB logging of key "losses_after_forward" was ignored as its type "<class 'torch.Tensor'>" is not handled by this wrapper.
WARNING 2025-07-04 13:10:09 db_utils.py:141 WandB logging of key "losses_after_in_ep_bound" was ignored as its type "<class 'torch.Tensor'>" is not handled by this wrapper.
WARNING 2025-07-04 13:10:09 db_utils.py:141 WandB logging of key "losses_after_rm_padding" was ignored as its type "<class 'torch.Tensor'>" is not handled by this wrapper.
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
INFO 2025-07-04 13:14:20 ts/train.py:232 step:5K smpl:125K ep:19 epch:9.36 loss:0.078 grdn:0.908 lr:2.3e-05 updt_s:1.236 data_s:0.016
WARNING 2025-07-04 13:14:20 db_utils.py:141 WandB logging of key "losses_after_forward" was ignored as its type "<class 'torch.Tensor'>" is not handled by this wrapper.
WARNING 2025-07-04 13:14:20 db_utils.py:141 WandB logging of key "losses_after_in_ep_bound" was ignored as its type "<class 'torch.Tensor'>" is not handled by this wrapper.
WARNING 2025-07-04 13:14:20 db_utils.py:141 WandB logging of key "losses_after_rm_padding" was ignored as its type "<class 'torch.Tensor'>" is not handled by this wrapper.
INFO 2025-07-04 13:18:28 ts/train.py:232 step:5K smpl:130K ep:19 epch:9.72 loss:0.073 grdn:0.939 lr:2.3e-05 updt_s:1.238 data_s:0.000
WARNING 2025-07-04 13:18:28 db_utils.py:141 WandB logging of key "losses_after_forward" was ignored as its type "<class 'torch.Tensor'>" is not handled by this wrapper.
WARNING 2025-07-04 13:18:28 db_utils.py:141 WandB logging of key "losses_after_in_ep_bound" was ignored as its type "<class 'torch.Tensor'>" is not handled by this wrapper.
WARNING 2025-07-04 13:18:28 db_utils.py:141 WandB logging of key "losses_after_rm_padding" was ignored as its type "<class 'torch.Tensor'>" is not handled by this wrapper.
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
INFO 2025-07-04 13:22:39 ts/train.py:232 step:6K smpl:134K ep:20 epch:10.08 loss:0.067 grdn:0.894 lr:2.3e-05 updt_s:1.236 data_s:0.017
WARNING 2025-07-04 13:22:39 db_utils.py:141 WandB logging of key "losses_after_forward" was ignored as its type "<class 'torch.Tensor'>" is not handled by this wrapper.
WARNING 2025-07-04 13:22:39 db_utils.py:141 WandB logging of key "losses_after_in_ep_bound" was ignored as its type "<class 'torch.Tensor'>" is not handled by this wrapper.
WARNING 2025-07-04 13:22:39 db_utils.py:141 WandB logging of key "losses_after_rm_padding" was ignored as its type "<class 'torch.Tensor'>" is not handled by this wrapper.
INFO 2025-07-04 13:26:47 ts/train.py:232 step:6K smpl:139K ep:21 epch:10.44 loss:0.063 grdn:0.888 lr:2.3e-05 updt_s:1.239 data_s:0.000
WARNING 2025-07-04 13:26:47 db_utils.py:141 WandB logging of key "losses_after_forward" was ignored as its type "<class 'torch.Tensor'>" is not handled by this wrapper.
WARNING 2025-07-04 13:26:47 db_utils.py:141 WandB logging of key "losses_after_in_ep_bound" was ignored as its type "<class 'torch.Tensor'>" is not handled by this wrapper.
WARNING 2025-07-04 13:26:47 db_utils.py:141 WandB logging of key "losses_after_rm_padding" was ignored as its type "<class 'torch.Tensor'>" is not handled by this wrapper.
INFO 2025-07-04 13:30:55 ts/train.py:232 step:6K smpl:144K ep:22 epch:10.80 loss:0.060 grdn:0.857 lr:2.3e-05 updt_s:1.239 data_s:0.000
WARNING 2025-07-04 13:30:55 db_utils.py:141 WandB logging of key "losses_after_forward" was ignored as its type "<class 'torch.Tensor'>" is not handled by this wrapper.
WARNING 2025-07-04 13:30:55 db_utils.py:141 WandB logging of key "losses_after_in_ep_bound" was ignored as its type "<class 'torch.Tensor'>" is not handled by this wrapper.
WARNING 2025-07-04 13:30:55 db_utils.py:141 WandB logging of key "losses_after_rm_padding" was ignored as its type "<class 'torch.Tensor'>" is not handled by this wrapper.
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
INFO 2025-07-04 13:35:06 ts/train.py:232 step:6K smpl:149K ep:22 epch:11.16 loss:0.054 grdn:0.825 lr:2.3e-05 updt_s:1.237 data_s:0.016
WARNING 2025-07-04 13:35:06 db_utils.py:141 WandB logging of key "losses_after_forward" was ignored as its type "<class 'torch.Tensor'>" is not handled by this wrapper.
WARNING 2025-07-04 13:35:06 db_utils.py:141 WandB logging of key "losses_after_in_ep_bound" was ignored as its type "<class 'torch.Tensor'>" is not handled by this wrapper.
WARNING 2025-07-04 13:35:06 db_utils.py:141 WandB logging of key "losses_after_rm_padding" was ignored as its type "<class 'torch.Tensor'>" is not handled by this wrapper.
INFO 2025-07-04 13:39:14 ts/train.py:232 step:6K smpl:154K ep:23 epch:11.52 loss:0.050 grdn:0.828 lr:2.3e-05 updt_s:1.240 data_s:0.000
WARNING 2025-07-04 13:39:14 db_utils.py:141 WandB logging of key "losses_after_forward" was ignored as its type "<class 'torch.Tensor'>" is not handled by this wrapper.
WARNING 2025-07-04 13:39:14 db_utils.py:141 WandB logging of key "losses_after_in_ep_bound" was ignored as its type "<class 'torch.Tensor'>" is not handled by this wrapper.
WARNING 2025-07-04 13:39:14 db_utils.py:141 WandB logging of key "losses_after_rm_padding" was ignored as its type "<class 'torch.Tensor'>" is not handled by this wrapper.
INFO 2025-07-04 13:43:22 ts/train.py:232 step:7K smpl:158K ep:24 epch:11.88 loss:0.048 grdn:0.824 lr:2.2e-05 updt_s:1.239 data_s:0.000
WARNING 2025-07-04 13:43:22 db_utils.py:141 WandB logging of key "losses_after_forward" was ignored as its type "<class 'torch.Tensor'>" is not handled by this wrapper.
WARNING 2025-07-04 13:43:22 db_utils.py:141 WandB logging of key "losses_after_in_ep_bound" was ignored as its type "<class 'torch.Tensor'>" is not handled by this wrapper.
WARNING 2025-07-04 13:43:22 db_utils.py:141 WandB logging of key "losses_after_rm_padding" was ignored as its type "<class 'torch.Tensor'>" is not handled by this wrapper.
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
INFO 2025-07-04 13:47:33 ts/train.py:232 step:7K smpl:163K ep:24 epch:12.24 loss:0.045 grdn:0.803 lr:2.2e-05 updt_s:1.236 data_s:0.016
WARNING 2025-07-04 13:47:33 db_utils.py:141 WandB logging of key "losses_after_forward" was ignored as its type "<class 'torch.Tensor'>" is not handled by this wrapper.
WARNING 2025-07-04 13:47:33 db_utils.py:141 WandB logging of key "losses_after_in_ep_bound" was ignored as its type "<class 'torch.Tensor'>" is not handled by this wrapper.
WARNING 2025-07-04 13:47:33 db_utils.py:141 WandB logging of key "losses_after_rm_padding" was ignored as its type "<class 'torch.Tensor'>" is not handled by this wrapper.
INFO 2025-07-04 13:51:41 ts/train.py:232 step:7K smpl:168K ep:25 epch:12.60 loss:0.043 grdn:0.793 lr:2.2e-05 updt_s:1.238 data_s:0.000
WARNING 2025-07-04 13:51:41 db_utils.py:141 WandB logging of key "losses_after_forward" was ignored as its type "<class 'torch.Tensor'>" is not handled by this wrapper.
WARNING 2025-07-04 13:51:41 db_utils.py:141 WandB logging of key "losses_after_in_ep_bound" was ignored as its type "<class 'torch.Tensor'>" is not handled by this wrapper.
WARNING 2025-07-04 13:51:41 db_utils.py:141 WandB logging of key "losses_after_rm_padding" was ignored as its type "<class 'torch.Tensor'>" is not handled by this wrapper.
INFO 2025-07-04 13:55:49 ts/train.py:232 step:7K smpl:173K ep:26 epch:12.96 loss:0.041 grdn:0.780 lr:2.2e-05 updt_s:1.237 data_s:0.000
WARNING 2025-07-04 13:55:49 db_utils.py:141 WandB logging of key "losses_after_forward" was ignored as its type "<class 'torch.Tensor'>" is not handled by this wrapper.
WARNING 2025-07-04 13:55:49 db_utils.py:141 WandB logging of key "losses_after_in_ep_bound" was ignored as its type "<class 'torch.Tensor'>" is not handled by this wrapper.
WARNING 2025-07-04 13:55:49 db_utils.py:141 WandB logging of key "losses_after_rm_padding" was ignored as its type "<class 'torch.Tensor'>" is not handled by this wrapper.
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
INFO 2025-07-04 13:59:59 ts/train.py:232 step:7K smpl:178K ep:27 epch:13.32 loss:0.038 grdn:0.762 lr:2.2e-05 updt_s:1.236 data_s:0.017
WARNING 2025-07-04 13:59:59 db_utils.py:141 WandB logging of key "losses_after_forward" was ignored as its type "<class 'torch.Tensor'>" is not handled by this wrapper.
WARNING 2025-07-04 13:59:59 db_utils.py:141 WandB logging of key "losses_after_in_ep_bound" was ignored as its type "<class 'torch.Tensor'>" is not handled by this wrapper.
WARNING 2025-07-04 13:59:59 db_utils.py:141 WandB logging of key "losses_after_rm_padding" was ignored as its type "<class 'torch.Tensor'>" is not handled by this wrapper.
INFO 2025-07-04 14:04:07 ts/train.py:232 step:8K smpl:182K ep:27 epch:13.68 loss:0.038 grdn:0.747 lr:2.2e-05 updt_s:1.239 data_s:0.000
WARNING 2025-07-04 14:04:07 db_utils.py:141 WandB logging of key "losses_after_forward" was ignored as its type "<class 'torch.Tensor'>" is not handled by this wrapper.
WARNING 2025-07-04 14:04:07 db_utils.py:141 WandB logging of key "losses_after_in_ep_bound" was ignored as its type "<class 'torch.Tensor'>" is not handled by this wrapper.
WARNING 2025-07-04 14:04:07 db_utils.py:141 WandB logging of key "losses_after_rm_padding" was ignored as its type "<class 'torch.Tensor'>" is not handled by this wrapper.
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
INFO 2025-07-04 14:08:19 ts/train.py:232 step:8K smpl:187K ep:28 epch:14.04 loss:0.036 grdn:0.760 lr:2.2e-05 updt_s:1.238 data_s:0.017
WARNING 2025-07-04 14:08:19 db_utils.py:141 WandB logging of key "losses_after_forward" was ignored as its type "<class 'torch.Tensor'>" is not handled by this wrapper.
WARNING 2025-07-04 14:08:19 db_utils.py:141 WandB logging of key "losses_after_in_ep_bound" was ignored as its type "<class 'torch.Tensor'>" is not handled by this wrapper.
WARNING 2025-07-04 14:08:19 db_utils.py:141 WandB logging of key "losses_after_rm_padding" was ignored as its type "<class 'torch.Tensor'>" is not handled by this wrapper.
INFO 2025-07-04 14:12:27 ts/train.py:232 step:8K smpl:192K ep:29 epch:14.40 loss:0.035 grdn:0.740 lr:2.1e-05 updt_s:1.241 data_s:0.000
WARNING 2025-07-04 14:12:27 db_utils.py:141 WandB logging of key "losses_after_forward" was ignored as its type "<class 'torch.Tensor'>" is not handled by this wrapper.
WARNING 2025-07-04 14:12:27 db_utils.py:141 WandB logging of key "losses_after_in_ep_bound" was ignored as its type "<class 'torch.Tensor'>" is not handled by this wrapper.
WARNING 2025-07-04 14:12:27 db_utils.py:141 WandB logging of key "losses_after_rm_padding" was ignored as its type "<class 'torch.Tensor'>" is not handled by this wrapper.
INFO 2025-07-04 14:16:35 ts/train.py:232 step:8K smpl:197K ep:30 epch:14.76 loss:0.034 grdn:0.748 lr:2.1e-05 updt_s:1.240 data_s:0.000
WARNING 2025-07-04 14:16:35 db_utils.py:141 WandB logging of key "losses_after_forward" was ignored as its type "<class 'torch.Tensor'>" is not handled by this wrapper.
WARNING 2025-07-04 14:16:35 db_utils.py:141 WandB logging of key "losses_after_in_ep_bound" was ignored as its type "<class 'torch.Tensor'>" is not handled by this wrapper.
WARNING 2025-07-04 14:16:35 db_utils.py:141 WandB logging of key "losses_after_rm_padding" was ignored as its type "<class 'torch.Tensor'>" is not handled by this wrapper.
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
INFO 2025-07-04 14:20:47 ts/train.py:232 step:8K smpl:202K ep:30 epch:15.12 loss:0.033 grdn:0.726 lr:2.1e-05 updt_s:1.238 data_s:0.016
WARNING 2025-07-04 14:20:47 db_utils.py:141 WandB logging of key "losses_after_forward" was ignored as its type "<class 'torch.Tensor'>" is not handled by this wrapper.
WARNING 2025-07-04 14:20:47 db_utils.py:141 WandB logging of key "losses_after_in_ep_bound" was ignored as its type "<class 'torch.Tensor'>" is not handled by this wrapper.
WARNING 2025-07-04 14:20:47 db_utils.py:141 WandB logging of key "losses_after_rm_padding" was ignored as its type "<class 'torch.Tensor'>" is not handled by this wrapper.
INFO 2025-07-04 14:24:55 ts/train.py:232 step:9K smpl:206K ep:31 epch:15.48 loss:0.032 grdn:0.714 lr:2.1e-05 updt_s:1.242 data_s:0.000
WARNING 2025-07-04 14:24:55 db_utils.py:141 WandB logging of key "losses_after_forward" was ignored as its type "<class 'torch.Tensor'>" is not handled by this wrapper.
WARNING 2025-07-04 14:24:55 db_utils.py:141 WandB logging of key "losses_after_in_ep_bound" was ignored as its type "<class 'torch.Tensor'>" is not handled by this wrapper.
WARNING 2025-07-04 14:24:55 db_utils.py:141 WandB logging of key "losses_after_rm_padding" was ignored as its type "<class 'torch.Tensor'>" is not handled by this wrapper.
INFO 2025-07-04 14:29:04 ts/train.py:232 step:9K smpl:211K ep:32 epch:15.84 loss:0.030 grdn:0.685 lr:2.1e-05 updt_s:1.243 data_s:0.000
WARNING 2025-07-04 14:29:04 db_utils.py:141 WandB logging of key "losses_after_forward" was ignored as its type "<class 'torch.Tensor'>" is not handled by this wrapper.
WARNING 2025-07-04 14:29:04 db_utils.py:141 WandB logging of key "losses_after_in_ep_bound" was ignored as its type "<class 'torch.Tensor'>" is not handled by this wrapper.
WARNING 2025-07-04 14:29:04 db_utils.py:141 WandB logging of key "losses_after_rm_padding" was ignored as its type "<class 'torch.Tensor'>" is not handled by this wrapper.
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
INFO 2025-07-04 14:33:16 ts/train.py:232 step:9K smpl:216K ep:32 epch:16.20 loss:0.030 grdn:0.705 lr:2.0e-05 updt_s:1.238 data_s:0.016
WARNING 2025-07-04 14:33:16 db_utils.py:141 WandB logging of key "losses_after_forward" was ignored as its type "<class 'torch.Tensor'>" is not handled by this wrapper.
WARNING 2025-07-04 14:33:16 db_utils.py:141 WandB logging of key "losses_after_in_ep_bound" was ignored as its type "<class 'torch.Tensor'>" is not handled by this wrapper.
WARNING 2025-07-04 14:33:16 db_utils.py:141 WandB logging of key "losses_after_rm_padding" was ignored as its type "<class 'torch.Tensor'>" is not handled by this wrapper.
INFO 2025-07-04 14:37:23 ts/train.py:232 step:9K smpl:221K ep:33 epch:16.56 loss:0.028 grdn:0.679 lr:2.0e-05 updt_s:1.238 data_s:0.000
WARNING 2025-07-04 14:37:23 db_utils.py:141 WandB logging of key "losses_after_forward" was ignored as its type "<class 'torch.Tensor'>" is not handled by this wrapper.
WARNING 2025-07-04 14:37:23 db_utils.py:141 WandB logging of key "losses_after_in_ep_bound" was ignored as its type "<class 'torch.Tensor'>" is not handled by this wrapper.
WARNING 2025-07-04 14:37:23 db_utils.py:141 WandB logging of key "losses_after_rm_padding" was ignored as its type "<class 'torch.Tensor'>" is not handled by this wrapper.
INFO 2025-07-04 14:41:31 ts/train.py:232 step:9K smpl:226K ep:34 epch:16.92 loss:0.028 grdn:0.678 lr:2.0e-05 updt_s:1.238 data_s:0.000
WARNING 2025-07-04 14:41:31 db_utils.py:141 WandB logging of key "losses_after_forward" was ignored as its type "<class 'torch.Tensor'>" is not handled by this wrapper.
WARNING 2025-07-04 14:41:31 db_utils.py:141 WandB logging of key "losses_after_in_ep_bound" was ignored as its type "<class 'torch.Tensor'>" is not handled by this wrapper.
WARNING 2025-07-04 14:41:31 db_utils.py:141 WandB logging of key "losses_after_rm_padding" was ignored as its type "<class 'torch.Tensor'>" is not handled by this wrapper.
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
INFO 2025-07-04 14:45:43 ts/train.py:232 step:10K smpl:230K ep:35 epch:17.28 loss:0.027 grdn:0.672 lr:2.0e-05 updt_s:1.238 data_s:0.016
WARNING 2025-07-04 14:45:43 db_utils.py:141 WandB logging of key "losses_after_forward" was ignored as its type "<class 'torch.Tensor'>" is not handled by this wrapper.
WARNING 2025-07-04 14:45:43 db_utils.py:141 WandB logging of key "losses_after_in_ep_bound" was ignored as its type "<class 'torch.Tensor'>" is not handled by this wrapper.
WARNING 2025-07-04 14:45:43 db_utils.py:141 WandB logging of key "losses_after_rm_padding" was ignored as its type "<class 'torch.Tensor'>" is not handled by this wrapper.
INFO 2025-07-04 14:49:51 ts/train.py:232 step:10K smpl:235K ep:35 epch:17.64 loss:0.026 grdn:0.651 lr:2.0e-05 updt_s:1.241 data_s:0.000
WARNING 2025-07-04 14:49:51 db_utils.py:141 WandB logging of key "losses_after_forward" was ignored as its type "<class 'torch.Tensor'>" is not handled by this wrapper.
WARNING 2025-07-04 14:49:51 db_utils.py:141 WandB logging of key "losses_after_in_ep_bound" was ignored as its type "<class 'torch.Tensor'>" is not handled by this wrapper.
WARNING 2025-07-04 14:49:51 db_utils.py:141 WandB logging of key "losses_after_rm_padding" was ignored as its type "<class 'torch.Tensor'>" is not handled by this wrapper.
INFO 2025-07-04 14:54:00 ts/train.py:232 step:10K smpl:240K ep:36 epch:18.00 loss:0.026 grdn:0.679 lr:1.9e-05 updt_s:1.241 data_s:0.000
WARNING 2025-07-04 14:54:00 db_utils.py:141 WandB logging of key "losses_after_forward" was ignored as its type "<class 'torch.Tensor'>" is not handled by this wrapper.
WARNING 2025-07-04 14:54:00 db_utils.py:141 WandB logging of key "losses_after_in_ep_bound" was ignored as its type "<class 'torch.Tensor'>" is not handled by this wrapper.
WARNING 2025-07-04 14:54:00 db_utils.py:141 WandB logging of key "losses_after_rm_padding" was ignored as its type "<class 'torch.Tensor'>" is not handled by this wrapper.
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
INFO 2025-07-04 14:58:11 ts/train.py:232 step:10K smpl:245K ep:37 epch:18.36 loss:0.025 grdn:0.646 lr:1.9e-05 updt_s:1.236 data_s:0.016
WARNING 2025-07-04 14:58:11 db_utils.py:141 WandB logging of key "losses_after_forward" was ignored as its type "<class 'torch.Tensor'>" is not handled by this wrapper.
WARNING 2025-07-04 14:58:11 db_utils.py:141 WandB logging of key "losses_after_in_ep_bound" was ignored as its type "<class 'torch.Tensor'>" is not handled by this wrapper.
WARNING 2025-07-04 14:58:11 db_utils.py:141 WandB logging of key "losses_after_rm_padding" was ignored as its type "<class 'torch.Tensor'>" is not handled by this wrapper.
INFO 2025-07-04 15:02:19 ts/train.py:232 step:10K smpl:250K ep:37 epch:18.72 loss:0.024 grdn:0.633 lr:1.9e-05 updt_s:1.239 data_s:0.000
WARNING 2025-07-04 15:02:19 db_utils.py:141 WandB logging of key "losses_after_forward" was ignored as its type "<class 'torch.Tensor'>" is not handled by this wrapper.
WARNING 2025-07-04 15:02:19 db_utils.py:141 WandB logging of key "losses_after_in_ep_bound" was ignored as its type "<class 'torch.Tensor'>" is not handled by this wrapper.
WARNING 2025-07-04 15:02:19 db_utils.py:141 WandB logging of key "losses_after_rm_padding" was ignored as its type "<class 'torch.Tensor'>" is not handled by this wrapper.
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
INFO 2025-07-04 15:06:30 ts/train.py:232 step:11K smpl:254K ep:38 epch:19.08 loss:0.025 grdn:0.646 lr:1.9e-05 updt_s:1.236 data_s:0.016
WARNING 2025-07-04 15:06:30 db_utils.py:141 WandB logging of key "losses_after_forward" was ignored as its type "<class 'torch.Tensor'>" is not handled by this wrapper.
WARNING 2025-07-04 15:06:30 db_utils.py:141 WandB logging of key "losses_after_in_ep_bound" was ignored as its type "<class 'torch.Tensor'>" is not handled by this wrapper.
WARNING 2025-07-04 15:06:30 db_utils.py:141 WandB logging of key "losses_after_rm_padding" was ignored as its type "<class 'torch.Tensor'>" is not handled by this wrapper.
INFO 2025-07-04 15:10:38 ts/train.py:232 step:11K smpl:259K ep:39 epch:19.44 loss:0.024 grdn:0.642 lr:1.9e-05 updt_s:1.241 data_s:0.000
WARNING 2025-07-04 15:10:38 db_utils.py:141 WandB logging of key "losses_after_forward" was ignored as its type "<class 'torch.Tensor'>" is not handled by this wrapper.
WARNING 2025-07-04 15:10:38 db_utils.py:141 WandB logging of key "losses_after_in_ep_bound" was ignored as its type "<class 'torch.Tensor'>" is not handled by this wrapper.
WARNING 2025-07-04 15:10:38 db_utils.py:141 WandB logging of key "losses_after_rm_padding" was ignored as its type "<class 'torch.Tensor'>" is not handled by this wrapper.
INFO 2025-07-04 15:14:47 ts/train.py:232 step:11K smpl:264K ep:40 epch:19.80 loss:0.023 grdn:0.632 lr:1.8e-05 updt_s:1.241 data_s:0.000
WARNING 2025-07-04 15:14:47 db_utils.py:141 WandB logging of key "losses_after_forward" was ignored as its type "<class 'torch.Tensor'>" is not handled by this wrapper.
WARNING 2025-07-04 15:14:47 db_utils.py:141 WandB logging of key "losses_after_in_ep_bound" was ignored as its type "<class 'torch.Tensor'>" is not handled by this wrapper.
WARNING 2025-07-04 15:14:47 db_utils.py:141 WandB logging of key "losses_after_rm_padding" was ignored as its type "<class 'torch.Tensor'>" is not handled by this wrapper.
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
INFO 2025-07-04 15:18:57 ts/train.py:232 step:11K smpl:269K ep:40 epch:20.16 loss:0.023 grdn:0.626 lr:1.8e-05 updt_s:1.236 data_s:0.016
WARNING 2025-07-04 15:18:57 db_utils.py:141 WandB logging of key "losses_after_forward" was ignored as its type "<class 'torch.Tensor'>" is not handled by this wrapper.
WARNING 2025-07-04 15:18:57 db_utils.py:141 WandB logging of key "losses_after_in_ep_bound" was ignored as its type "<class 'torch.Tensor'>" is not handled by this wrapper.
WARNING 2025-07-04 15:18:57 db_utils.py:141 WandB logging of key "losses_after_rm_padding" was ignored as its type "<class 'torch.Tensor'>" is not handled by this wrapper.
INFO 2025-07-04 15:23:05 ts/train.py:232 step:11K smpl:274K ep:41 epch:20.52 loss:0.022 grdn:0.610 lr:1.8e-05 updt_s:1.236 data_s:0.000
WARNING 2025-07-04 15:23:05 db_utils.py:141 WandB logging of key "losses_after_forward" was ignored as its type "<class 'torch.Tensor'>" is not handled by this wrapper.
WARNING 2025-07-04 15:23:05 db_utils.py:141 WandB logging of key "losses_after_in_ep_bound" was ignored as its type "<class 'torch.Tensor'>" is not handled by this wrapper.
WARNING 2025-07-04 15:23:05 db_utils.py:141 WandB logging of key "losses_after_rm_padding" was ignored as its type "<class 'torch.Tensor'>" is not handled by this wrapper.
INFO 2025-07-04 15:27:12 ts/train.py:232 step:12K smpl:278K ep:42 epch:20.88 loss:0.022 grdn:0.613 lr:1.8e-05 updt_s:1.236 data_s:0.000
WARNING 2025-07-04 15:27:12 db_utils.py:141 WandB logging of key "losses_after_forward" was ignored as its type "<class 'torch.Tensor'>" is not handled by this wrapper.
WARNING 2025-07-04 15:27:12 db_utils.py:141 WandB logging of key "losses_after_in_ep_bound" was ignored as its type "<class 'torch.Tensor'>" is not handled by this wrapper.
WARNING 2025-07-04 15:27:12 db_utils.py:141 WandB logging of key "losses_after_rm_padding" was ignored as its type "<class 'torch.Tensor'>" is not handled by this wrapper.
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
INFO 2025-07-04 15:31:24 ts/train.py:232 step:12K smpl:283K ep:42 epch:21.24 loss:0.023 grdn:0.614 lr:1.8e-05 updt_s:1.235 data_s:0.022
WARNING 2025-07-04 15:31:24 db_utils.py:141 WandB logging of key "losses_after_forward" was ignored as its type "<class 'torch.Tensor'>" is not handled by this wrapper.
WARNING 2025-07-04 15:31:24 db_utils.py:141 WandB logging of key "losses_after_in_ep_bound" was ignored as its type "<class 'torch.Tensor'>" is not handled by this wrapper.
WARNING 2025-07-04 15:31:24 db_utils.py:141 WandB logging of key "losses_after_rm_padding" was ignored as its type "<class 'torch.Tensor'>" is not handled by this wrapper.
INFO 2025-07-04 15:35:32 ts/train.py:232 step:12K smpl:288K ep:43 epch:21.60 loss:0.021 grdn:0.605 lr:1.7e-05 updt_s:1.238 data_s:0.000
WARNING 2025-07-04 15:35:32 db_utils.py:141 WandB logging of key "losses_after_forward" was ignored as its type "<class 'torch.Tensor'>" is not handled by this wrapper.
WARNING 2025-07-04 15:35:32 db_utils.py:141 WandB logging of key "losses_after_in_ep_bound" was ignored as its type "<class 'torch.Tensor'>" is not handled by this wrapper.
WARNING 2025-07-04 15:35:32 db_utils.py:141 WandB logging of key "losses_after_rm_padding" was ignored as its type "<class 'torch.Tensor'>" is not handled by this wrapper.
INFO 2025-07-04 15:39:40 ts/train.py:232 step:12K smpl:293K ep:44 epch:21.96 loss:0.021 grdn:0.602 lr:1.7e-05 updt_s:1.238 data_s:0.000
WARNING 2025-07-04 15:39:40 db_utils.py:141 WandB logging of key "losses_after_forward" was ignored as its type "<class 'torch.Tensor'>" is not handled by this wrapper.
WARNING 2025-07-04 15:39:40 db_utils.py:141 WandB logging of key "losses_after_in_ep_bound" was ignored as its type "<class 'torch.Tensor'>" is not handled by this wrapper.
WARNING 2025-07-04 15:39:40 db_utils.py:141 WandB logging of key "losses_after_rm_padding" was ignored as its type "<class 'torch.Tensor'>" is not handled by this wrapper.
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
INFO 2025-07-04 15:43:53 ts/train.py:232 step:12K smpl:298K ep:45 epch:22.32 loss:0.021 grdn:0.608 lr:1.7e-05 updt_s:1.236 data_s:0.026
WARNING 2025-07-04 15:43:53 db_utils.py:141 WandB logging of key "losses_after_forward" was ignored as its type "<class 'torch.Tensor'>" is not handled by this wrapper.
WARNING 2025-07-04 15:43:53 db_utils.py:141 WandB logging of key "losses_after_in_ep_bound" was ignored as its type "<class 'torch.Tensor'>" is not handled by this wrapper.
WARNING 2025-07-04 15:43:53 db_utils.py:141 WandB logging of key "losses_after_rm_padding" was ignored as its type "<class 'torch.Tensor'>" is not handled by this wrapper.
INFO 2025-07-04 15:48:01 ts/train.py:232 step:13K smpl:302K ep:45 epch:22.68 loss:0.020 grdn:0.580 lr:1.7e-05 updt_s:1.239 data_s:0.000
WARNING 2025-07-04 15:48:01 db_utils.py:141 WandB logging of key "losses_after_forward" was ignored as its type "<class 'torch.Tensor'>" is not handled by this wrapper.
WARNING 2025-07-04 15:48:01 db_utils.py:141 WandB logging of key "losses_after_in_ep_bound" was ignored as its type "<class 'torch.Tensor'>" is not handled by this wrapper.
WARNING 2025-07-04 15:48:01 db_utils.py:141 WandB logging of key "losses_after_rm_padding" was ignored as its type "<class 'torch.Tensor'>" is not handled by this wrapper.
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
INFO 2025-07-04 15:52:11 ts/train.py:232 step:13K smpl:307K ep:46 epch:23.04 loss:0.020 grdn:0.589 lr:1.6e-05 updt_s:1.234 data_s:0.016
WARNING 2025-07-04 15:52:11 db_utils.py:141 WandB logging of key "losses_after_forward" was ignored as its type "<class 'torch.Tensor'>" is not handled by this wrapper.
WARNING 2025-07-04 15:52:11 db_utils.py:141 WandB logging of key "losses_after_in_ep_bound" was ignored as its type "<class 'torch.Tensor'>" is not handled by this wrapper.
WARNING 2025-07-04 15:52:11 db_utils.py:141 WandB logging of key "losses_after_rm_padding" was ignored as its type "<class 'torch.Tensor'>" is not handled by this wrapper.
INFO 2025-07-04 15:56:19 ts/train.py:232 step:13K smpl:312K ep:47 epch:23.40 loss:0.019 grdn:0.569 lr:1.6e-05 updt_s:1.235 data_s:0.000
WARNING 2025-07-04 15:56:19 db_utils.py:141 WandB logging of key "losses_after_forward" was ignored as its type "<class 'torch.Tensor'>" is not handled by this wrapper.
WARNING 2025-07-04 15:56:19 db_utils.py:141 WandB logging of key "losses_after_in_ep_bound" was ignored as its type "<class 'torch.Tensor'>" is not handled by this wrapper.
WARNING 2025-07-04 15:56:19 db_utils.py:141 WandB logging of key "losses_after_rm_padding" was ignored as its type "<class 'torch.Tensor'>" is not handled by this wrapper.
INFO 2025-07-04 16:00:26 ts/train.py:232 step:13K smpl:317K ep:48 epch:23.76 loss:0.019 grdn:0.579 lr:1.6e-05 updt_s:1.236 data_s:0.000
WARNING 2025-07-04 16:00:26 db_utils.py:141 WandB logging of key "losses_after_forward" was ignored as its type "<class 'torch.Tensor'>" is not handled by this wrapper.
WARNING 2025-07-04 16:00:26 db_utils.py:141 WandB logging of key "losses_after_in_ep_bound" was ignored as its type "<class 'torch.Tensor'>" is not handled by this wrapper.
WARNING 2025-07-04 16:00:26 db_utils.py:141 WandB logging of key "losses_after_rm_padding" was ignored as its type "<class 'torch.Tensor'>" is not handled by this wrapper.
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
INFO 2025-07-04 16:04:37 ts/train.py:232 step:13K smpl:322K ep:48 epch:24.12 loss:0.019 grdn:0.576 lr:1.6e-05 updt_s:1.235 data_s:0.021
WARNING 2025-07-04 16:04:37 db_utils.py:141 WandB logging of key "losses_after_forward" was ignored as its type "<class 'torch.Tensor'>" is not handled by this wrapper.
WARNING 2025-07-04 16:04:37 db_utils.py:141 WandB logging of key "losses_after_in_ep_bound" was ignored as its type "<class 'torch.Tensor'>" is not handled by this wrapper.
WARNING 2025-07-04 16:04:37 db_utils.py:141 WandB logging of key "losses_after_rm_padding" was ignored as its type "<class 'torch.Tensor'>" is not handled by this wrapper.
INFO 2025-07-04 16:08:45 ts/train.py:232 step:14K smpl:326K ep:49 epch:24.48 loss:0.018 grdn:0.557 lr:1.6e-05 updt_s:1.237 data_s:0.000
WARNING 2025-07-04 16:08:45 db_utils.py:141 WandB logging of key "losses_after_forward" was ignored as its type "<class 'torch.Tensor'>" is not handled by this wrapper.
WARNING 2025-07-04 16:08:45 db_utils.py:141 WandB logging of key "losses_after_in_ep_bound" was ignored as its type "<class 'torch.Tensor'>" is not handled by this wrapper.
WARNING 2025-07-04 16:08:45 db_utils.py:141 WandB logging of key "losses_after_rm_padding" was ignored as its type "<class 'torch.Tensor'>" is not handled by this wrapper.
INFO 2025-07-04 16:12:53 ts/train.py:232 step:14K smpl:331K ep:50 epch:24.84 loss:0.017 grdn:0.553 lr:1.5e-05 updt_s:1.238 data_s:0.000
WARNING 2025-07-04 16:12:53 db_utils.py:141 WandB logging of key "losses_after_forward" was ignored as its type "<class 'torch.Tensor'>" is not handled by this wrapper.
WARNING 2025-07-04 16:12:53 db_utils.py:141 WandB logging of key "losses_after_in_ep_bound" was ignored as its type "<class 'torch.Tensor'>" is not handled by this wrapper.
WARNING 2025-07-04 16:12:53 db_utils.py:141 WandB logging of key "losses_after_rm_padding" was ignored as its type "<class 'torch.Tensor'>" is not handled by this wrapper.
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
INFO 2025-07-04 16:17:04 ts/train.py:232 step:14K smpl:336K ep:50 epch:25.20 loss:0.018 grdn:0.563 lr:1.5e-05 updt_s:1.235 data_s:0.017
WARNING 2025-07-04 16:17:04 db_utils.py:141 WandB logging of key "losses_after_forward" was ignored as its type "<class 'torch.Tensor'>" is not handled by this wrapper.
WARNING 2025-07-04 16:17:04 db_utils.py:141 WandB logging of key "losses_after_in_ep_bound" was ignored as its type "<class 'torch.Tensor'>" is not handled by this wrapper.
WARNING 2025-07-04 16:17:04 db_utils.py:141 WandB logging of key "losses_after_rm_padding" was ignored as its type "<class 'torch.Tensor'>" is not handled by this wrapper.
INFO 2025-07-04 16:21:11 ts/train.py:232 step:14K smpl:341K ep:51 epch:25.56 loss:0.017 grdn:0.550 lr:1.5e-05 updt_s:1.236 data_s:0.000
WARNING 2025-07-04 16:21:11 db_utils.py:141 WandB logging of key "losses_after_forward" was ignored as its type "<class 'torch.Tensor'>" is not handled by this wrapper.
WARNING 2025-07-04 16:21:11 db_utils.py:141 WandB logging of key "losses_after_in_ep_bound" was ignored as its type "<class 'torch.Tensor'>" is not handled by this wrapper.
WARNING 2025-07-04 16:21:11 db_utils.py:141 WandB logging of key "losses_after_rm_padding" was ignored as its type "<class 'torch.Tensor'>" is not handled by this wrapper.
INFO 2025-07-04 16:25:19 ts/train.py:232 step:14K smpl:346K ep:52 epch:25.92 loss:0.017 grdn:0.555 lr:1.5e-05 updt_s:1.236 data_s:0.000
WARNING 2025-07-04 16:25:19 db_utils.py:141 WandB logging of key "losses_after_forward" was ignored as its type "<class 'torch.Tensor'>" is not handled by this wrapper.
WARNING 2025-07-04 16:25:19 db_utils.py:141 WandB logging of key "losses_after_in_ep_bound" was ignored as its type "<class 'torch.Tensor'>" is not handled by this wrapper.
WARNING 2025-07-04 16:25:19 db_utils.py:141 WandB logging of key "losses_after_rm_padding" was ignored as its type "<class 'torch.Tensor'>" is not handled by this wrapper.
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
INFO 2025-07-04 16:29:29 ts/train.py:232 step:15K smpl:350K ep:53 epch:26.28 loss:0.017 grdn:0.536 lr:1.4e-05 updt_s:1.234 data_s:0.016
WARNING 2025-07-04 16:29:29 db_utils.py:141 WandB logging of key "losses_after_forward" was ignored as its type "<class 'torch.Tensor'>" is not handled by this wrapper.
WARNING 2025-07-04 16:29:29 db_utils.py:141 WandB logging of key "losses_after_in_ep_bound" was ignored as its type "<class 'torch.Tensor'>" is not handled by this wrapper.
WARNING 2025-07-04 16:29:29 db_utils.py:141 WandB logging of key "losses_after_rm_padding" was ignored as its type "<class 'torch.Tensor'>" is not handled by this wrapper.
INFO 2025-07-04 16:33:36 ts/train.py:232 step:15K smpl:355K ep:53 epch:26.64 loss:0.017 grdn:0.541 lr:1.4e-05 updt_s:1.236 data_s:0.000
WARNING 2025-07-04 16:33:36 db_utils.py:141 WandB logging of key "losses_after_forward" was ignored as its type "<class 'torch.Tensor'>" is not handled by this wrapper.
WARNING 2025-07-04 16:33:36 db_utils.py:141 WandB logging of key "losses_after_in_ep_bound" was ignored as its type "<class 'torch.Tensor'>" is not handled by this wrapper.
WARNING 2025-07-04 16:33:36 db_utils.py:141 WandB logging of key "losses_after_rm_padding" was ignored as its type "<class 'torch.Tensor'>" is not handled by this wrapper.
INFO 2025-07-04 16:37:44 ts/train.py:232 step:15K smpl:360K ep:54 epch:27.00 loss:0.016 grdn:0.543 lr:1.4e-05 updt_s:1.236 data_s:0.000
WARNING 2025-07-04 16:37:44 db_utils.py:141 WandB logging of key "losses_after_forward" was ignored as its type "<class 'torch.Tensor'>" is not handled by this wrapper.
WARNING 2025-07-04 16:37:44 db_utils.py:141 WandB logging of key "losses_after_in_ep_bound" was ignored as its type "<class 'torch.Tensor'>" is not handled by this wrapper.
WARNING 2025-07-04 16:37:44 db_utils.py:141 WandB logging of key "losses_after_rm_padding" was ignored as its type "<class 'torch.Tensor'>" is not handled by this wrapper.
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
INFO 2025-07-04 16:41:54 ts/train.py:232 step:15K smpl:365K ep:55 epch:27.36 loss:0.016 grdn:0.522 lr:1.4e-05 updt_s:1.236 data_s:0.015
WARNING 2025-07-04 16:41:54 db_utils.py:141 WandB logging of key "losses_after_forward" was ignored as its type "<class 'torch.Tensor'>" is not handled by this wrapper.
WARNING 2025-07-04 16:41:54 db_utils.py:141 WandB logging of key "losses_after_in_ep_bound" was ignored as its type "<class 'torch.Tensor'>" is not handled by this wrapper.
WARNING 2025-07-04 16:41:54 db_utils.py:141 WandB logging of key "losses_after_rm_padding" was ignored as its type "<class 'torch.Tensor'>" is not handled by this wrapper.
INFO 2025-07-04 16:46:02 ts/train.py:232 step:15K smpl:370K ep:55 epch:27.72 loss:0.016 grdn:0.528 lr:1.3e-05 updt_s:1.238 data_s:0.000
WARNING 2025-07-04 16:46:02 db_utils.py:141 WandB logging of key "losses_after_forward" was ignored as its type "<class 'torch.Tensor'>" is not handled by this wrapper.
WARNING 2025-07-04 16:46:02 db_utils.py:141 WandB logging of key "losses_after_in_ep_bound" was ignored as its type "<class 'torch.Tensor'>" is not handled by this wrapper.
WARNING 2025-07-04 16:46:02 db_utils.py:141 WandB logging of key "losses_after_rm_padding" was ignored as its type "<class 'torch.Tensor'>" is not handled by this wrapper.
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
INFO 2025-07-04 16:50:13 ts/train.py:232 step:16K smpl:374K ep:56 epch:28.08 loss:0.016 grdn:0.530 lr:1.3e-05 updt_s:1.237 data_s:0.016
WARNING 2025-07-04 16:50:13 db_utils.py:141 WandB logging of key "losses_after_forward" was ignored as its type "<class 'torch.Tensor'>" is not handled by this wrapper.
WARNING 2025-07-04 16:50:13 db_utils.py:141 WandB logging of key "losses_after_in_ep_bound" was ignored as its type "<class 'torch.Tensor'>" is not handled by this wrapper.
WARNING 2025-07-04 16:50:13 db_utils.py:141 WandB logging of key "losses_after_rm_padding" was ignored as its type "<class 'torch.Tensor'>" is not handled by this wrapper.
INFO 2025-07-04 16:54:22 ts/train.py:232 step:16K smpl:379K ep:57 epch:28.44 loss:0.015 grdn:0.523 lr:1.3e-05 updt_s:1.240 data_s:0.000
WARNING 2025-07-04 16:54:22 db_utils.py:141 WandB logging of key "losses_after_forward" was ignored as its type "<class 'torch.Tensor'>" is not handled by this wrapper.
WARNING 2025-07-04 16:54:22 db_utils.py:141 WandB logging of key "losses_after_in_ep_bound" was ignored as its type "<class 'torch.Tensor'>" is not handled by this wrapper.
WARNING 2025-07-04 16:54:22 db_utils.py:141 WandB logging of key "losses_after_rm_padding" was ignored as its type "<class 'torch.Tensor'>" is not handled by this wrapper.
INFO 2025-07-04 16:58:30 ts/train.py:232 step:16K smpl:384K ep:58 epch:28.80 loss:0.015 grdn:0.506 lr:1.3e-05 updt_s:1.239 data_s:0.000
WARNING 2025-07-04 16:58:30 db_utils.py:141 WandB logging of key "losses_after_forward" was ignored as its type "<class 'torch.Tensor'>" is not handled by this wrapper.
WARNING 2025-07-04 16:58:30 db_utils.py:141 WandB logging of key "losses_after_in_ep_bound" was ignored as its type "<class 'torch.Tensor'>" is not handled by this wrapper.
WARNING 2025-07-04 16:58:30 db_utils.py:141 WandB logging of key "losses_after_rm_padding" was ignored as its type "<class 'torch.Tensor'>" is not handled by this wrapper.
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
INFO 2025-07-04 17:02:41 ts/train.py:232 step:16K smpl:389K ep:58 epch:29.16 loss:0.014 grdn:0.511 lr:1.2e-05 updt_s:1.237 data_s:0.015
WARNING 2025-07-04 17:02:41 db_utils.py:141 WandB logging of key "losses_after_forward" was ignored as its type "<class 'torch.Tensor'>" is not handled by this wrapper.
WARNING 2025-07-04 17:02:41 db_utils.py:141 WandB logging of key "losses_after_in_ep_bound" was ignored as its type "<class 'torch.Tensor'>" is not handled by this wrapper.
WARNING 2025-07-04 17:02:41 db_utils.py:141 WandB logging of key "losses_after_rm_padding" was ignored as its type "<class 'torch.Tensor'>" is not handled by this wrapper.
INFO 2025-07-04 17:06:49 ts/train.py:232 step:16K smpl:394K ep:59 epch:29.52 loss:0.014 grdn:0.514 lr:1.2e-05 updt_s:1.240 data_s:0.000
WARNING 2025-07-04 17:06:49 db_utils.py:141 WandB logging of key "losses_after_forward" was ignored as its type "<class 'torch.Tensor'>" is not handled by this wrapper.
WARNING 2025-07-04 17:06:49 db_utils.py:141 WandB logging of key "losses_after_in_ep_bound" was ignored as its type "<class 'torch.Tensor'>" is not handled by this wrapper.
WARNING 2025-07-04 17:06:49 db_utils.py:141 WandB logging of key "losses_after_rm_padding" was ignored as its type "<class 'torch.Tensor'>" is not handled by this wrapper.
INFO 2025-07-04 17:10:57 ts/train.py:232 step:17K smpl:398K ep:60 epch:29.88 loss:0.014 grdn:0.514 lr:1.2e-05 updt_s:1.240 data_s:0.000
WARNING 2025-07-04 17:10:57 db_utils.py:141 WandB logging of key "losses_after_forward" was ignored as its type "<class 'torch.Tensor'>" is not handled by this wrapper.
WARNING 2025-07-04 17:10:57 db_utils.py:141 WandB logging of key "losses_after_in_ep_bound" was ignored as its type "<class 'torch.Tensor'>" is not handled by this wrapper.
WARNING 2025-07-04 17:10:57 db_utils.py:141 WandB logging of key "losses_after_rm_padding" was ignored as its type "<class 'torch.Tensor'>" is not handled by this wrapper.
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
INFO 2025-07-04 17:15:08 ts/train.py:232 step:17K smpl:403K ep:60 epch:30.24 loss:0.015 grdn:0.510 lr:1.2e-05 updt_s:1.238 data_s:0.016
WARNING 2025-07-04 17:15:08 db_utils.py:141 WandB logging of key "losses_after_forward" was ignored as its type "<class 'torch.Tensor'>" is not handled by this wrapper.
WARNING 2025-07-04 17:15:08 db_utils.py:141 WandB logging of key "losses_after_in_ep_bound" was ignored as its type "<class 'torch.Tensor'>" is not handled by this wrapper.
WARNING 2025-07-04 17:15:08 db_utils.py:141 WandB logging of key "losses_after_rm_padding" was ignored as its type "<class 'torch.Tensor'>" is not handled by this wrapper.
INFO 2025-07-04 17:19:17 ts/train.py:232 step:17K smpl:408K ep:61 epch:30.60 loss:0.014 grdn:0.500 lr:1.2e-05 updt_s:1.241 data_s:0.000
WARNING 2025-07-04 17:19:17 db_utils.py:141 WandB logging of key "losses_after_forward" was ignored as its type "<class 'torch.Tensor'>" is not handled by this wrapper.
WARNING 2025-07-04 17:19:17 db_utils.py:141 WandB logging of key "losses_after_in_ep_bound" was ignored as its type "<class 'torch.Tensor'>" is not handled by this wrapper.
WARNING 2025-07-04 17:19:17 db_utils.py:141 WandB logging of key "losses_after_rm_padding" was ignored as its type "<class 'torch.Tensor'>" is not handled by this wrapper.
INFO 2025-07-04 17:23:26 ts/train.py:232 step:17K smpl:413K ep:62 epch:30.96 loss:0.015 grdn:0.510 lr:1.1e-05 updt_s:1.242 data_s:0.000
WARNING 2025-07-04 17:23:26 db_utils.py:141 WandB logging of key "losses_after_forward" was ignored as its type "<class 'torch.Tensor'>" is not handled by this wrapper.
WARNING 2025-07-04 17:23:26 db_utils.py:141 WandB logging of key "losses_after_in_ep_bound" was ignored as its type "<class 'torch.Tensor'>" is not handled by this wrapper.
WARNING 2025-07-04 17:23:26 db_utils.py:141 WandB logging of key "losses_after_rm_padding" was ignored as its type "<class 'torch.Tensor'>" is not handled by this wrapper.
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
INFO 2025-07-04 17:27:36 ts/train.py:232 step:17K smpl:418K ep:63 epch:31.32 loss:0.013 grdn:0.489 lr:1.1e-05 updt_s:1.236 data_s:0.016
WARNING 2025-07-04 17:27:36 db_utils.py:141 WandB logging of key "losses_after_forward" was ignored as its type "<class 'torch.Tensor'>" is not handled by this wrapper.
WARNING 2025-07-04 17:27:36 db_utils.py:141 WandB logging of key "losses_after_in_ep_bound" was ignored as its type "<class 'torch.Tensor'>" is not handled by this wrapper.
WARNING 2025-07-04 17:27:36 db_utils.py:141 WandB logging of key "losses_after_rm_padding" was ignored as its type "<class 'torch.Tensor'>" is not handled by this wrapper.
INFO 2025-07-04 17:31:44 ts/train.py:232 step:18K smpl:422K ep:63 epch:31.68 loss:0.014 grdn:0.494 lr:1.1e-05 updt_s:1.237 data_s:0.000
WARNING 2025-07-04 17:31:44 db_utils.py:141 WandB logging of key "losses_after_forward" was ignored as its type "<class 'torch.Tensor'>" is not handled by this wrapper.
WARNING 2025-07-04 17:31:44 db_utils.py:141 WandB logging of key "losses_after_in_ep_bound" was ignored as its type "<class 'torch.Tensor'>" is not handled by this wrapper.
WARNING 2025-07-04 17:31:44 db_utils.py:141 WandB logging of key "losses_after_rm_padding" was ignored as its type "<class 'torch.Tensor'>" is not handled by this wrapper.
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
INFO 2025-07-04 17:35:54 ts/train.py:232 step:18K smpl:427K ep:64 epch:32.04 loss:0.014 grdn:0.490 lr:1.1e-05 updt_s:1.234 data_s:0.014
WARNING 2025-07-04 17:35:54 db_utils.py:141 WandB logging of key "losses_after_forward" was ignored as its type "<class 'torch.Tensor'>" is not handled by this wrapper.
WARNING 2025-07-04 17:35:54 db_utils.py:141 WandB logging of key "losses_after_in_ep_bound" was ignored as its type "<class 'torch.Tensor'>" is not handled by this wrapper.
WARNING 2025-07-04 17:35:54 db_utils.py:141 WandB logging of key "losses_after_rm_padding" was ignored as its type "<class 'torch.Tensor'>" is not handled by this wrapper.
INFO 2025-07-04 17:40:01 ts/train.py:232 step:18K smpl:432K ep:65 epch:32.40 loss:0.013 grdn:0.487 lr:1.0e-05 updt_s:1.237 data_s:0.000
WARNING 2025-07-04 17:40:01 db_utils.py:141 WandB logging of key "losses_after_forward" was ignored as its type "<class 'torch.Tensor'>" is not handled by this wrapper.
WARNING 2025-07-04 17:40:01 db_utils.py:141 WandB logging of key "losses_after_in_ep_bound" was ignored as its type "<class 'torch.Tensor'>" is not handled by this wrapper.
WARNING 2025-07-04 17:40:01 db_utils.py:141 WandB logging of key "losses_after_rm_padding" was ignored as its type "<class 'torch.Tensor'>" is not handled by this wrapper.
INFO 2025-07-04 17:44:09 ts/train.py:232 step:18K smpl:437K ep:66 epch:32.76 loss:0.014 grdn:0.503 lr:1.0e-05 updt_s:1.237 data_s:0.000
WARNING 2025-07-04 17:44:09 db_utils.py:141 WandB logging of key "losses_after_forward" was ignored as its type "<class 'torch.Tensor'>" is not handled by this wrapper.
WARNING 2025-07-04 17:44:09 db_utils.py:141 WandB logging of key "losses_after_in_ep_bound" was ignored as its type "<class 'torch.Tensor'>" is not handled by this wrapper.
WARNING 2025-07-04 17:44:09 db_utils.py:141 WandB logging of key "losses_after_rm_padding" was ignored as its type "<class 'torch.Tensor'>" is not handled by this wrapper.
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
INFO 2025-07-04 17:48:19 ts/train.py:232 step:18K smpl:442K ep:66 epch:33.12 loss:0.013 grdn:0.484 lr:9.9e-06 updt_s:1.237 data_s:0.014
WARNING 2025-07-04 17:48:19 db_utils.py:141 WandB logging of key "losses_after_forward" was ignored as its type "<class 'torch.Tensor'>" is not handled by this wrapper.
WARNING 2025-07-04 17:48:19 db_utils.py:141 WandB logging of key "losses_after_in_ep_bound" was ignored as its type "<class 'torch.Tensor'>" is not handled by this wrapper.
WARNING 2025-07-04 17:48:19 db_utils.py:141 WandB logging of key "losses_after_rm_padding" was ignored as its type "<class 'torch.Tensor'>" is not handled by this wrapper.
INFO 2025-07-04 17:52:28 ts/train.py:232 step:19K smpl:446K ep:67 epch:33.48 loss:0.013 grdn:0.474 lr:9.7e-06 updt_s:1.239 data_s:0.000
WARNING 2025-07-04 17:52:28 db_utils.py:141 WandB logging of key "losses_after_forward" was ignored as its type "<class 'torch.Tensor'>" is not handled by this wrapper.
WARNING 2025-07-04 17:52:28 db_utils.py:141 WandB logging of key "losses_after_in_ep_bound" was ignored as its type "<class 'torch.Tensor'>" is not handled by this wrapper.
WARNING 2025-07-04 17:52:28 db_utils.py:141 WandB logging of key "losses_after_rm_padding" was ignored as its type "<class 'torch.Tensor'>" is not handled by this wrapper.
INFO 2025-07-04 17:56:36 ts/train.py:232 step:19K smpl:451K ep:68 epch:33.84 loss:0.013 grdn:0.484 lr:9.5e-06 updt_s:1.239 data_s:0.000
WARNING 2025-07-04 17:56:36 db_utils.py:141 WandB logging of key "losses_after_forward" was ignored as its type "<class 'torch.Tensor'>" is not handled by this wrapper.
WARNING 2025-07-04 17:56:36 db_utils.py:141 WandB logging of key "losses_after_in_ep_bound" was ignored as its type "<class 'torch.Tensor'>" is not handled by this wrapper.
WARNING 2025-07-04 17:56:36 db_utils.py:141 WandB logging of key "losses_after_rm_padding" was ignored as its type "<class 'torch.Tensor'>" is not handled by this wrapper.
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
INFO 2025-07-04 18:00:46 ts/train.py:232 step:19K smpl:456K ep:68 epch:34.20 loss:0.013 grdn:0.484 lr:9.3e-06 updt_s:1.235 data_s:0.014
WARNING 2025-07-04 18:00:46 db_utils.py:141 WandB logging of key "losses_after_forward" was ignored as its type "<class 'torch.Tensor'>" is not handled by this wrapper.
WARNING 2025-07-04 18:00:46 db_utils.py:141 WandB logging of key "losses_after_in_ep_bound" was ignored as its type "<class 'torch.Tensor'>" is not handled by this wrapper.
WARNING 2025-07-04 18:00:46 db_utils.py:141 WandB logging of key "losses_after_rm_padding" was ignored as its type "<class 'torch.Tensor'>" is not handled by this wrapper.
INFO 2025-07-04 18:04:53 ts/train.py:232 step:19K smpl:461K ep:69 epch:34.56 loss:0.013 grdn:0.468 lr:9.1e-06 updt_s:1.236 data_s:0.000
WARNING 2025-07-04 18:04:53 db_utils.py:141 WandB logging of key "losses_after_forward" was ignored as its type "<class 'torch.Tensor'>" is not handled by this wrapper.
WARNING 2025-07-04 18:04:53 db_utils.py:141 WandB logging of key "losses_after_in_ep_bound" was ignored as its type "<class 'torch.Tensor'>" is not handled by this wrapper.
WARNING 2025-07-04 18:04:53 db_utils.py:141 WandB logging of key "losses_after_rm_padding" was ignored as its type "<class 'torch.Tensor'>" is not handled by this wrapper.
INFO 2025-07-04 18:09:00 ts/train.py:232 step:19K smpl:466K ep:70 epch:34.92 loss:0.013 grdn:0.467 lr:8.9e-06 updt_s:1.235 data_s:0.000
WARNING 2025-07-04 18:09:00 db_utils.py:141 WandB logging of key "losses_after_forward" was ignored as its type "<class 'torch.Tensor'>" is not handled by this wrapper.
WARNING 2025-07-04 18:09:00 db_utils.py:141 WandB logging of key "losses_after_in_ep_bound" was ignored as its type "<class 'torch.Tensor'>" is not handled by this wrapper.
WARNING 2025-07-04 18:09:00 db_utils.py:141 WandB logging of key "losses_after_rm_padding" was ignored as its type "<class 'torch.Tensor'>" is not handled by this wrapper.
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
INFO 2025-07-04 18:13:10 ts/train.py:232 step:20K smpl:470K ep:71 epch:35.28 loss:0.012 grdn:0.461 lr:8.6e-06 updt_s:1.234 data_s:0.014
WARNING 2025-07-04 18:13:10 db_utils.py:141 WandB logging of key "losses_after_forward" was ignored as its type "<class 'torch.Tensor'>" is not handled by this wrapper.
WARNING 2025-07-04 18:13:10 db_utils.py:141 WandB logging of key "losses_after_in_ep_bound" was ignored as its type "<class 'torch.Tensor'>" is not handled by this wrapper.
WARNING 2025-07-04 18:13:10 db_utils.py:141 WandB logging of key "losses_after_rm_padding" was ignored as its type "<class 'torch.Tensor'>" is not handled by this wrapper.
INFO 2025-07-04 18:17:18 ts/train.py:232 step:20K smpl:475K ep:71 epch:35.64 loss:0.012 grdn:0.461 lr:8.4e-06 updt_s:1.237 data_s:0.000
WARNING 2025-07-04 18:17:18 db_utils.py:141 WandB logging of key "losses_after_forward" was ignored as its type "<class 'torch.Tensor'>" is not handled by this wrapper.
WARNING 2025-07-04 18:17:18 db_utils.py:141 WandB logging of key "losses_after_in_ep_bound" was ignored as its type "<class 'torch.Tensor'>" is not handled by this wrapper.
WARNING 2025-07-04 18:17:18 db_utils.py:141 WandB logging of key "losses_after_rm_padding" was ignored as its type "<class 'torch.Tensor'>" is not handled by this wrapper.
INFO 2025-07-04 18:21:26 ts/train.py:232 step:20K smpl:480K ep:72 epch:36.00 loss:0.012 grdn:0.465 lr:8.2e-06 updt_s:1.237 data_s:0.000
WARNING 2025-07-04 18:21:26 db_utils.py:141 WandB logging of key "losses_after_forward" was ignored as its type "<class 'torch.Tensor'>" is not handled by this wrapper.
WARNING 2025-07-04 18:21:26 db_utils.py:141 WandB logging of key "losses_after_in_ep_bound" was ignored as its type "<class 'torch.Tensor'>" is not handled by this wrapper.
WARNING 2025-07-04 18:21:26 db_utils.py:141 WandB logging of key "losses_after_rm_padding" was ignored as its type "<class 'torch.Tensor'>" is not handled by this wrapper.
INFO 2025-07-04 18:21:26 ts/train.py:241 Checkpoint policy after step 20000
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
INFO 2025-07-04 18:26:24 ts/train.py:232 step:20K smpl:485K ep:73 epch:36.36 loss:0.012 grdn:0.448 lr:8.0e-06 updt_s:1.234 data_s:0.017
WARNING 2025-07-04 18:26:24 db_utils.py:141 WandB logging of key "losses_after_forward" was ignored as its type "<class 'torch.Tensor'>" is not handled by this wrapper.
WARNING 2025-07-04 18:26:24 db_utils.py:141 WandB logging of key "losses_after_in_ep_bound" was ignored as its type "<class 'torch.Tensor'>" is not handled by this wrapper.
WARNING 2025-07-04 18:26:24 db_utils.py:141 WandB logging of key "losses_after_rm_padding" was ignored as its type "<class 'torch.Tensor'>" is not handled by this wrapper.
INFO 2025-07-04 18:30:32 ts/train.py:232 step:20K smpl:490K ep:73 epch:36.72 loss:0.012 grdn:0.465 lr:7.8e-06 updt_s:1.237 data_s:0.000
WARNING 2025-07-04 18:30:32 db_utils.py:141 WandB logging of key "losses_after_forward" was ignored as its type "<class 'torch.Tensor'>" is not handled by this wrapper.
WARNING 2025-07-04 18:30:32 db_utils.py:141 WandB logging of key "losses_after_in_ep_bound" was ignored as its type "<class 'torch.Tensor'>" is not handled by this wrapper.
WARNING 2025-07-04 18:30:32 db_utils.py:141 WandB logging of key "losses_after_rm_padding" was ignored as its type "<class 'torch.Tensor'>" is not handled by this wrapper.
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
INFO 2025-07-04 18:34:43 ts/train.py:232 step:21K smpl:494K ep:74 epch:37.08 loss:0.012 grdn:0.453 lr:7.6e-06 updt_s:1.236 data_s:0.016
WARNING 2025-07-04 18:34:43 db_utils.py:141 WandB logging of key "losses_after_forward" was ignored as its type "<class 'torch.Tensor'>" is not handled by this wrapper.
WARNING 2025-07-04 18:34:43 db_utils.py:141 WandB logging of key "losses_after_in_ep_bound" was ignored as its type "<class 'torch.Tensor'>" is not handled by this wrapper.
WARNING 2025-07-04 18:34:43 db_utils.py:141 WandB logging of key "losses_after_rm_padding" was ignored as its type "<class 'torch.Tensor'>" is not handled by this wrapper.
INFO 2025-07-04 18:38:50 ts/train.py:232 step:21K smpl:499K ep:75 epch:37.44 loss:0.012 grdn:0.452 lr:7.4e-06 updt_s:1.237 data_s:0.000
WARNING 2025-07-04 18:38:50 db_utils.py:141 WandB logging of key "losses_after_forward" was ignored as its type "<class 'torch.Tensor'>" is not handled by this wrapper.
WARNING 2025-07-04 18:38:50 db_utils.py:141 WandB logging of key "losses_after_in_ep_bound" was ignored as its type "<class 'torch.Tensor'>" is not handled by this wrapper.
WARNING 2025-07-04 18:38:50 db_utils.py:141 WandB logging of key "losses_after_rm_padding" was ignored as its type "<class 'torch.Tensor'>" is not handled by this wrapper.
INFO 2025-07-04 18:42:59 ts/train.py:232 step:21K smpl:504K ep:76 epch:37.80 loss:0.012 grdn:0.443 lr:7.2e-06 updt_s:1.238 data_s:0.000
WARNING 2025-07-04 18:42:59 db_utils.py:141 WandB logging of key "losses_after_forward" was ignored as its type "<class 'torch.Tensor'>" is not handled by this wrapper.
WARNING 2025-07-04 18:42:59 db_utils.py:141 WandB logging of key "losses_after_in_ep_bound" was ignored as its type "<class 'torch.Tensor'>" is not handled by this wrapper.
WARNING 2025-07-04 18:42:59 db_utils.py:141 WandB logging of key "losses_after_rm_padding" was ignored as its type "<class 'torch.Tensor'>" is not handled by this wrapper.
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
INFO 2025-07-04 18:47:09 ts/train.py:232 step:21K smpl:509K ep:76 epch:38.16 loss:0.012 grdn:0.451 lr:7.0e-06 updt_s:1.236 data_s:0.014
WARNING 2025-07-04 18:47:09 db_utils.py:141 WandB logging of key "losses_after_forward" was ignored as its type "<class 'torch.Tensor'>" is not handled by this wrapper.
WARNING 2025-07-04 18:47:09 db_utils.py:141 WandB logging of key "losses_after_in_ep_bound" was ignored as its type "<class 'torch.Tensor'>" is not handled by this wrapper.
WARNING 2025-07-04 18:47:09 db_utils.py:141 WandB logging of key "losses_after_rm_padding" was ignored as its type "<class 'torch.Tensor'>" is not handled by this wrapper.
INFO 2025-07-04 18:51:17 ts/train.py:232 step:21K smpl:514K ep:77 epch:38.52 loss:0.012 grdn:0.446 lr:6.9e-06 updt_s:1.238 data_s:0.000
WARNING 2025-07-04 18:51:17 db_utils.py:141 WandB logging of key "losses_after_forward" was ignored as its type "<class 'torch.Tensor'>" is not handled by this wrapper.
WARNING 2025-07-04 18:51:17 db_utils.py:141 WandB logging of key "losses_after_in_ep_bound" was ignored as its type "<class 'torch.Tensor'>" is not handled by this wrapper.
WARNING 2025-07-04 18:51:17 db_utils.py:141 WandB logging of key "losses_after_rm_padding" was ignored as its type "<class 'torch.Tensor'>" is not handled by this wrapper.
INFO 2025-07-04 18:55:25 ts/train.py:232 step:22K smpl:518K ep:78 epch:38.88 loss:0.011 grdn:0.435 lr:6.7e-06 updt_s:1.237 data_s:0.000
WARNING 2025-07-04 18:55:25 db_utils.py:141 WandB logging of key "losses_after_forward" was ignored as its type "<class 'torch.Tensor'>" is not handled by this wrapper.
WARNING 2025-07-04 18:55:25 db_utils.py:141 WandB logging of key "losses_after_in_ep_bound" was ignored as its type "<class 'torch.Tensor'>" is not handled by this wrapper.
WARNING 2025-07-04 18:55:25 db_utils.py:141 WandB logging of key "losses_after_rm_padding" was ignored as its type "<class 'torch.Tensor'>" is not handled by this wrapper.
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
INFO 2025-07-04 18:59:35 ts/train.py:232 step:22K smpl:523K ep:78 epch:39.24 loss:0.011 grdn:0.430 lr:6.5e-06 updt_s:1.235 data_s:0.014
WARNING 2025-07-04 18:59:35 db_utils.py:141 WandB logging of key "losses_after_forward" was ignored as its type "<class 'torch.Tensor'>" is not handled by this wrapper.
WARNING 2025-07-04 18:59:35 db_utils.py:141 WandB logging of key "losses_after_in_ep_bound" was ignored as its type "<class 'torch.Tensor'>" is not handled by this wrapper.
WARNING 2025-07-04 18:59:35 db_utils.py:141 WandB logging of key "losses_after_rm_padding" was ignored as its type "<class 'torch.Tensor'>" is not handled by this wrapper.
INFO 2025-07-04 19:03:42 ts/train.py:232 step:22K smpl:528K ep:79 epch:39.60 loss:0.012 grdn:0.447 lr:6.3e-06 updt_s:1.238 data_s:0.000
WARNING 2025-07-04 19:03:42 db_utils.py:141 WandB logging of key "losses_after_forward" was ignored as its type "<class 'torch.Tensor'>" is not handled by this wrapper.
WARNING 2025-07-04 19:03:42 db_utils.py:141 WandB logging of key "losses_after_in_ep_bound" was ignored as its type "<class 'torch.Tensor'>" is not handled by this wrapper.
WARNING 2025-07-04 19:03:42 db_utils.py:141 WandB logging of key "losses_after_rm_padding" was ignored as its type "<class 'torch.Tensor'>" is not handled by this wrapper.
INFO 2025-07-04 19:07:50 ts/train.py:232 step:22K smpl:533K ep:80 epch:39.96 loss:0.012 grdn:0.433 lr:6.1e-06 updt_s:1.237 data_s:0.000
WARNING 2025-07-04 19:07:50 db_utils.py:141 WandB logging of key "losses_after_forward" was ignored as its type "<class 'torch.Tensor'>" is not handled by this wrapper.
WARNING 2025-07-04 19:07:50 db_utils.py:141 WandB logging of key "losses_after_in_ep_bound" was ignored as its type "<class 'torch.Tensor'>" is not handled by this wrapper.
WARNING 2025-07-04 19:07:50 db_utils.py:141 WandB logging of key "losses_after_rm_padding" was ignored as its type "<class 'torch.Tensor'>" is not handled by this wrapper.
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
INFO 2025-07-04 19:12:00 ts/train.py:232 step:22K smpl:538K ep:81 epch:40.32 loss:0.011 grdn:0.420 lr:6.0e-06 updt_s:1.233 data_s:0.013
WARNING 2025-07-04 19:12:00 db_utils.py:141 WandB logging of key "losses_after_forward" was ignored as its type "<class 'torch.Tensor'>" is not handled by this wrapper.
WARNING 2025-07-04 19:12:00 db_utils.py:141 WandB logging of key "losses_after_in_ep_bound" was ignored as its type "<class 'torch.Tensor'>" is not handled by this wrapper.
WARNING 2025-07-04 19:12:00 db_utils.py:141 WandB logging of key "losses_after_rm_padding" was ignored as its type "<class 'torch.Tensor'>" is not handled by this wrapper.
INFO 2025-07-04 19:16:07 ts/train.py:232 step:23K smpl:542K ep:81 epch:40.68 loss:0.011 grdn:0.428 lr:5.8e-06 updt_s:1.236 data_s:0.000
WARNING 2025-07-04 19:16:07 db_utils.py:141 WandB logging of key "losses_after_forward" was ignored as its type "<class 'torch.Tensor'>" is not handled by this wrapper.
WARNING 2025-07-04 19:16:07 db_utils.py:141 WandB logging of key "losses_after_in_ep_bound" was ignored as its type "<class 'torch.Tensor'>" is not handled by this wrapper.
WARNING 2025-07-04 19:16:07 db_utils.py:141 WandB logging of key "losses_after_rm_padding" was ignored as its type "<class 'torch.Tensor'>" is not handled by this wrapper.
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
INFO 2025-07-04 19:20:17 ts/train.py:232 step:23K smpl:547K ep:82 epch:41.04 loss:0.011 grdn:0.428 lr:5.6e-06 updt_s:1.236 data_s:0.014
WARNING 2025-07-04 19:20:17 db_utils.py:141 WandB logging of key "losses_after_forward" was ignored as its type "<class 'torch.Tensor'>" is not handled by this wrapper.
WARNING 2025-07-04 19:20:17 db_utils.py:141 WandB logging of key "losses_after_in_ep_bound" was ignored as its type "<class 'torch.Tensor'>" is not handled by this wrapper.
WARNING 2025-07-04 19:20:17 db_utils.py:141 WandB logging of key "losses_after_rm_padding" was ignored as its type "<class 'torch.Tensor'>" is not handled by this wrapper.
INFO 2025-07-04 19:24:25 ts/train.py:232 step:23K smpl:552K ep:83 epch:41.40 loss:0.011 grdn:0.417 lr:5.5e-06 updt_s:1.238 data_s:0.000
WARNING 2025-07-04 19:24:25 db_utils.py:141 WandB logging of key "losses_after_forward" was ignored as its type "<class 'torch.Tensor'>" is not handled by this wrapper.
WARNING 2025-07-04 19:24:25 db_utils.py:141 WandB logging of key "losses_after_in_ep_bound" was ignored as its type "<class 'torch.Tensor'>" is not handled by this wrapper.
WARNING 2025-07-04 19:24:25 db_utils.py:141 WandB logging of key "losses_after_rm_padding" was ignored as its type "<class 'torch.Tensor'>" is not handled by this wrapper.
INFO 2025-07-04 19:28:33 ts/train.py:232 step:23K smpl:557K ep:84 epch:41.76 loss:0.011 grdn:0.423 lr:5.3e-06 updt_s:1.239 data_s:0.000
WARNING 2025-07-04 19:28:33 db_utils.py:141 WandB logging of key "losses_after_forward" was ignored as its type "<class 'torch.Tensor'>" is not handled by this wrapper.
WARNING 2025-07-04 19:28:33 db_utils.py:141 WandB logging of key "losses_after_in_ep_bound" was ignored as its type "<class 'torch.Tensor'>" is not handled by this wrapper.
WARNING 2025-07-04 19:28:33 db_utils.py:141 WandB logging of key "losses_after_rm_padding" was ignored as its type "<class 'torch.Tensor'>" is not handled by this wrapper.
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
INFO 2025-07-04 19:32:44 ts/train.py:232 step:23K smpl:562K ep:84 epch:42.12 loss:0.011 grdn:0.424 lr:5.2e-06 updt_s:1.236 data_s:0.016
WARNING 2025-07-04 19:32:44 db_utils.py:141 WandB logging of key "losses_after_forward" was ignored as its type "<class 'torch.Tensor'>" is not handled by this wrapper.
WARNING 2025-07-04 19:32:44 db_utils.py:141 WandB logging of key "losses_after_in_ep_bound" was ignored as its type "<class 'torch.Tensor'>" is not handled by this wrapper.
WARNING 2025-07-04 19:32:44 db_utils.py:141 WandB logging of key "losses_after_rm_padding" was ignored as its type "<class 'torch.Tensor'>" is not handled by this wrapper.
INFO 2025-07-04 19:36:52 ts/train.py:232 step:24K smpl:566K ep:85 epch:42.48 loss:0.011 grdn:0.431 lr:5.0e-06 updt_s:1.238 data_s:0.000
WARNING 2025-07-04 19:36:52 db_utils.py:141 WandB logging of key "losses_after_forward" was ignored as its type "<class 'torch.Tensor'>" is not handled by this wrapper.
WARNING 2025-07-04 19:36:52 db_utils.py:141 WandB logging of key "losses_after_in_ep_bound" was ignored as its type "<class 'torch.Tensor'>" is not handled by this wrapper.
WARNING 2025-07-04 19:36:52 db_utils.py:141 WandB logging of key "losses_after_rm_padding" was ignored as its type "<class 'torch.Tensor'>" is not handled by this wrapper.
INFO 2025-07-04 19:40:59 ts/train.py:232 step:24K smpl:571K ep:86 epch:42.84 loss:0.011 grdn:0.421 lr:4.9e-06 updt_s:1.238 data_s:0.000
WARNING 2025-07-04 19:40:59 db_utils.py:141 WandB logging of key "losses_after_forward" was ignored as its type "<class 'torch.Tensor'>" is not handled by this wrapper.
WARNING 2025-07-04 19:40:59 db_utils.py:141 WandB logging of key "losses_after_in_ep_bound" was ignored as its type "<class 'torch.Tensor'>" is not handled by this wrapper.
WARNING 2025-07-04 19:40:59 db_utils.py:141 WandB logging of key "losses_after_rm_padding" was ignored as its type "<class 'torch.Tensor'>" is not handled by this wrapper.
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
INFO 2025-07-04 19:45:09 ts/train.py:232 step:24K smpl:576K ep:86 epch:43.20 loss:0.011 grdn:0.421 lr:4.7e-06 updt_s:1.236 data_s:0.012
WARNING 2025-07-04 19:45:09 db_utils.py:141 WandB logging of key "losses_after_forward" was ignored as its type "<class 'torch.Tensor'>" is not handled by this wrapper.
WARNING 2025-07-04 19:45:09 db_utils.py:141 WandB logging of key "losses_after_in_ep_bound" was ignored as its type "<class 'torch.Tensor'>" is not handled by this wrapper.
WARNING 2025-07-04 19:45:09 db_utils.py:141 WandB logging of key "losses_after_rm_padding" was ignored as its type "<class 'torch.Tensor'>" is not handled by this wrapper.
INFO 2025-07-04 19:49:18 ts/train.py:232 step:24K smpl:581K ep:87 epch:43.56 loss:0.011 grdn:0.418 lr:4.6e-06 updt_s:1.241 data_s:0.000
WARNING 2025-07-04 19:49:18 db_utils.py:141 WandB logging of key "losses_after_forward" was ignored as its type "<class 'torch.Tensor'>" is not handled by this wrapper.
WARNING 2025-07-04 19:49:18 db_utils.py:141 WandB logging of key "losses_after_in_ep_bound" was ignored as its type "<class 'torch.Tensor'>" is not handled by this wrapper.
WARNING 2025-07-04 19:49:18 db_utils.py:141 WandB logging of key "losses_after_rm_padding" was ignored as its type "<class 'torch.Tensor'>" is not handled by this wrapper.
INFO 2025-07-04 19:53:26 ts/train.py:232 step:24K smpl:586K ep:88 epch:43.92 loss:0.010 grdn:0.418 lr:4.4e-06 updt_s:1.238 data_s:0.000
WARNING 2025-07-04 19:53:26 db_utils.py:141 WandB logging of key "losses_after_forward" was ignored as its type "<class 'torch.Tensor'>" is not handled by this wrapper.
WARNING 2025-07-04 19:53:26 db_utils.py:141 WandB logging of key "losses_after_in_ep_bound" was ignored as its type "<class 'torch.Tensor'>" is not handled by this wrapper.
WARNING 2025-07-04 19:53:26 db_utils.py:141 WandB logging of key "losses_after_rm_padding" was ignored as its type "<class 'torch.Tensor'>" is not handled by this wrapper.
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
INFO 2025-07-04 19:57:36 ts/train.py:232 step:25K smpl:590K ep:89 epch:44.28 loss:0.010 grdn:0.409 lr:4.3e-06 updt_s:1.234 data_s:0.013
WARNING 2025-07-04 19:57:36 db_utils.py:141 WandB logging of key "losses_after_forward" was ignored as its type "<class 'torch.Tensor'>" is not handled by this wrapper.
WARNING 2025-07-04 19:57:36 db_utils.py:141 WandB logging of key "losses_after_in_ep_bound" was ignored as its type "<class 'torch.Tensor'>" is not handled by this wrapper.
WARNING 2025-07-04 19:57:36 db_utils.py:141 WandB logging of key "losses_after_rm_padding" was ignored as its type "<class 'torch.Tensor'>" is not handled by this wrapper.
INFO 2025-07-04 20:01:43 ts/train.py:232 step:25K smpl:595K ep:89 epch:44.64 loss:0.010 grdn:0.414 lr:4.2e-06 updt_s:1.237 data_s:0.000
WARNING 2025-07-04 20:01:43 db_utils.py:141 WandB logging of key "losses_after_forward" was ignored as its type "<class 'torch.Tensor'>" is not handled by this wrapper.
WARNING 2025-07-04 20:01:43 db_utils.py:141 WandB logging of key "losses_after_in_ep_bound" was ignored as its type "<class 'torch.Tensor'>" is not handled by this wrapper.
WARNING 2025-07-04 20:01:43 db_utils.py:141 WandB logging of key "losses_after_rm_padding" was ignored as its type "<class 'torch.Tensor'>" is not handled by this wrapper.
INFO 2025-07-04 20:05:51 ts/train.py:232 step:25K smpl:600K ep:90 epch:45.00 loss:0.011 grdn:0.417 lr:4.1e-06 updt_s:1.237 data_s:0.000
WARNING 2025-07-04 20:05:51 db_utils.py:141 WandB logging of key "losses_after_forward" was ignored as its type "<class 'torch.Tensor'>" is not handled by this wrapper.
WARNING 2025-07-04 20:05:51 db_utils.py:141 WandB logging of key "losses_after_in_ep_bound" was ignored as its type "<class 'torch.Tensor'>" is not handled by this wrapper.
WARNING 2025-07-04 20:05:51 db_utils.py:141 WandB logging of key "losses_after_rm_padding" was ignored as its type "<class 'torch.Tensor'>" is not handled by this wrapper.
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
INFO 2025-07-04 20:10:01 ts/train.py:232 step:25K smpl:605K ep:91 epch:45.36 loss:0.011 grdn:0.419 lr:3.9e-06 updt_s:1.236 data_s:0.013
WARNING 2025-07-04 20:10:01 db_utils.py:141 WandB logging of key "losses_after_forward" was ignored as its type "<class 'torch.Tensor'>" is not handled by this wrapper.
WARNING 2025-07-04 20:10:01 db_utils.py:141 WandB logging of key "losses_after_in_ep_bound" was ignored as its type "<class 'torch.Tensor'>" is not handled by this wrapper.
WARNING 2025-07-04 20:10:01 db_utils.py:141 WandB logging of key "losses_after_rm_padding" was ignored as its type "<class 'torch.Tensor'>" is not handled by this wrapper.
INFO 2025-07-04 20:14:09 ts/train.py:232 step:25K smpl:610K ep:91 epch:45.72 loss:0.011 grdn:0.420 lr:3.8e-06 updt_s:1.237 data_s:0.000
WARNING 2025-07-04 20:14:09 db_utils.py:141 WandB logging of key "losses_after_forward" was ignored as its type "<class 'torch.Tensor'>" is not handled by this wrapper.
WARNING 2025-07-04 20:14:09 db_utils.py:141 WandB logging of key "losses_after_in_ep_bound" was ignored as its type "<class 'torch.Tensor'>" is not handled by this wrapper.
WARNING 2025-07-04 20:14:09 db_utils.py:141 WandB logging of key "losses_after_rm_padding" was ignored as its type "<class 'torch.Tensor'>" is not handled by this wrapper.
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
INFO 2025-07-04 20:18:19 ts/train.py:232 step:26K smpl:614K ep:92 epch:46.08 loss:0.011 grdn:0.416 lr:3.7e-06 updt_s:1.236 data_s:0.013
WARNING 2025-07-04 20:18:19 db_utils.py:141 WandB logging of key "losses_after_forward" was ignored as its type "<class 'torch.Tensor'>" is not handled by this wrapper.
WARNING 2025-07-04 20:18:19 db_utils.py:141 WandB logging of key "losses_after_in_ep_bound" was ignored as its type "<class 'torch.Tensor'>" is not handled by this wrapper.
WARNING 2025-07-04 20:18:19 db_utils.py:141 WandB logging of key "losses_after_rm_padding" was ignored as its type "<class 'torch.Tensor'>" is not handled by this wrapper.
INFO 2025-07-04 20:22:27 ts/train.py:232 step:26K smpl:619K ep:93 epch:46.44 loss:0.011 grdn:0.412 lr:3.6e-06 updt_s:1.239 data_s:0.000
WARNING 2025-07-04 20:22:27 db_utils.py:141 WandB logging of key "losses_after_forward" was ignored as its type "<class 'torch.Tensor'>" is not handled by this wrapper.
WARNING 2025-07-04 20:22:27 db_utils.py:141 WandB logging of key "losses_after_in_ep_bound" was ignored as its type "<class 'torch.Tensor'>" is not handled by this wrapper.
WARNING 2025-07-04 20:22:27 db_utils.py:141 WandB logging of key "losses_after_rm_padding" was ignored as its type "<class 'torch.Tensor'>" is not handled by this wrapper.
INFO 2025-07-04 20:26:35 ts/train.py:232 step:26K smpl:624K ep:94 epch:46.80 loss:0.011 grdn:0.408 lr:3.5e-06 updt_s:1.239 data_s:0.000
WARNING 2025-07-04 20:26:35 db_utils.py:141 WandB logging of key "losses_after_forward" was ignored as its type "<class 'torch.Tensor'>" is not handled by this wrapper.
WARNING 2025-07-04 20:26:35 db_utils.py:141 WandB logging of key "losses_after_in_ep_bound" was ignored as its type "<class 'torch.Tensor'>" is not handled by this wrapper.
WARNING 2025-07-04 20:26:35 db_utils.py:141 WandB logging of key "losses_after_rm_padding" was ignored as its type "<class 'torch.Tensor'>" is not handled by this wrapper.
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
INFO 2025-07-04 20:30:45 ts/train.py:232 step:26K smpl:629K ep:94 epch:47.16 loss:0.011 grdn:0.412 lr:3.4e-06 updt_s:1.234 data_s:0.013
WARNING 2025-07-04 20:30:45 db_utils.py:141 WandB logging of key "losses_after_forward" was ignored as its type "<class 'torch.Tensor'>" is not handled by this wrapper.
WARNING 2025-07-04 20:30:45 db_utils.py:141 WandB logging of key "losses_after_in_ep_bound" was ignored as its type "<class 'torch.Tensor'>" is not handled by this wrapper.
WARNING 2025-07-04 20:30:45 db_utils.py:141 WandB logging of key "losses_after_rm_padding" was ignored as its type "<class 'torch.Tensor'>" is not handled by this wrapper.
INFO 2025-07-04 20:34:52 ts/train.py:232 step:26K smpl:634K ep:95 epch:47.52 loss:0.010 grdn:0.407 lr:3.3e-06 updt_s:1.235 data_s:0.000
WARNING 2025-07-04 20:34:52 db_utils.py:141 WandB logging of key "losses_after_forward" was ignored as its type "<class 'torch.Tensor'>" is not handled by this wrapper.
WARNING 2025-07-04 20:34:52 db_utils.py:141 WandB logging of key "losses_after_in_ep_bound" was ignored as its type "<class 'torch.Tensor'>" is not handled by this wrapper.
WARNING 2025-07-04 20:34:52 db_utils.py:141 WandB logging of key "losses_after_rm_padding" was ignored as its type "<class 'torch.Tensor'>" is not handled by this wrapper.
INFO 2025-07-04 20:38:59 ts/train.py:232 step:27K smpl:638K ep:96 epch:47.88 loss:0.010 grdn:0.403 lr:3.2e-06 updt_s:1.235 data_s:0.000
WARNING 2025-07-04 20:38:59 db_utils.py:141 WandB logging of key "losses_after_forward" was ignored as its type "<class 'torch.Tensor'>" is not handled by this wrapper.
WARNING 2025-07-04 20:38:59 db_utils.py:141 WandB logging of key "losses_after_in_ep_bound" was ignored as its type "<class 'torch.Tensor'>" is not handled by this wrapper.
WARNING 2025-07-04 20:38:59 db_utils.py:141 WandB logging of key "losses_after_rm_padding" was ignored as its type "<class 'torch.Tensor'>" is not handled by this wrapper.
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
INFO 2025-07-04 20:43:09 ts/train.py:232 step:27K smpl:643K ep:96 epch:48.24 loss:0.010 grdn:0.404 lr:3.2e-06 updt_s:1.234 data_s:0.013
WARNING 2025-07-04 20:43:09 db_utils.py:141 WandB logging of key "losses_after_forward" was ignored as its type "<class 'torch.Tensor'>" is not handled by this wrapper.
WARNING 2025-07-04 20:43:09 db_utils.py:141 WandB logging of key "losses_after_in_ep_bound" was ignored as its type "<class 'torch.Tensor'>" is not handled by this wrapper.
WARNING 2025-07-04 20:43:09 db_utils.py:141 WandB logging of key "losses_after_rm_padding" was ignored as its type "<class 'torch.Tensor'>" is not handled by this wrapper.
INFO 2025-07-04 20:47:17 ts/train.py:232 step:27K smpl:648K ep:97 epch:48.60 loss:0.010 grdn:0.406 lr:3.1e-06 updt_s:1.237 data_s:0.000
WARNING 2025-07-04 20:47:17 db_utils.py:141 WandB logging of key "losses_after_forward" was ignored as its type "<class 'torch.Tensor'>" is not handled by this wrapper.
WARNING 2025-07-04 20:47:17 db_utils.py:141 WandB logging of key "losses_after_in_ep_bound" was ignored as its type "<class 'torch.Tensor'>" is not handled by this wrapper.
WARNING 2025-07-04 20:47:17 db_utils.py:141 WandB logging of key "losses_after_rm_padding" was ignored as its type "<class 'torch.Tensor'>" is not handled by this wrapper.
INFO 2025-07-04 20:51:25 ts/train.py:232 step:27K smpl:653K ep:98 epch:48.96 loss:0.011 grdn:0.411 lr:3.0e-06 updt_s:1.237 data_s:0.000
WARNING 2025-07-04 20:51:25 db_utils.py:141 WandB logging of key "losses_after_forward" was ignored as its type "<class 'torch.Tensor'>" is not handled by this wrapper.
WARNING 2025-07-04 20:51:25 db_utils.py:141 WandB logging of key "losses_after_in_ep_bound" was ignored as its type "<class 'torch.Tensor'>" is not handled by this wrapper.
WARNING 2025-07-04 20:51:25 db_utils.py:141 WandB logging of key "losses_after_rm_padding" was ignored as its type "<class 'torch.Tensor'>" is not handled by this wrapper.
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
INFO 2025-07-04 20:55:35 ts/train.py:232 step:27K smpl:658K ep:99 epch:49.32 loss:0.011 grdn:0.408 lr:2.9e-06 updt_s:1.235 data_s:0.013
WARNING 2025-07-04 20:55:35 db_utils.py:141 WandB logging of key "losses_after_forward" was ignored as its type "<class 'torch.Tensor'>" is not handled by this wrapper.
WARNING 2025-07-04 20:55:35 db_utils.py:141 WandB logging of key "losses_after_in_ep_bound" was ignored as its type "<class 'torch.Tensor'>" is not handled by this wrapper.
WARNING 2025-07-04 20:55:35 db_utils.py:141 WandB logging of key "losses_after_rm_padding" was ignored as its type "<class 'torch.Tensor'>" is not handled by this wrapper.
INFO 2025-07-04 20:59:42 ts/train.py:232 step:28K smpl:662K ep:99 epch:49.68 loss:0.010 grdn:0.407 lr:2.9e-06 updt_s:1.237 data_s:0.000
WARNING 2025-07-04 20:59:42 db_utils.py:141 WandB logging of key "losses_after_forward" was ignored as its type "<class 'torch.Tensor'>" is not handled by this wrapper.
WARNING 2025-07-04 20:59:42 db_utils.py:141 WandB logging of key "losses_after_in_ep_bound" was ignored as its type "<class 'torch.Tensor'>" is not handled by this wrapper.
WARNING 2025-07-04 20:59:42 db_utils.py:141 WandB logging of key "losses_after_rm_padding" was ignored as its type "<class 'torch.Tensor'>" is not handled by this wrapper.
INFO 2025-07-04 21:03:50 ts/train.py:232 step:28K smpl:667K ep:100 epch:50.04 loss:0.010 grdn:0.412 lr:2.8e-06 updt_s:1.235 data_s:0.000
WARNING 2025-07-04 21:03:50 db_utils.py:141 WandB logging of key "losses_after_forward" was ignored as its type "<class 'torch.Tensor'>" is not handled by this wrapper.
WARNING 2025-07-04 21:03:50 db_utils.py:141 WandB logging of key "losses_after_in_ep_bound" was ignored as its type "<class 'torch.Tensor'>" is not handled by this wrapper.
WARNING 2025-07-04 21:03:50 db_utils.py:141 WandB logging of key "losses_after_rm_padding" was ignored as its type "<class 'torch.Tensor'>" is not handled by this wrapper.
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
INFO 2025-07-04 21:08:00 ts/train.py:232 step:28K smpl:672K ep:101 epch:50.40 loss:0.011 grdn:0.419 lr:2.8e-06 updt_s:1.237 data_s:0.013
WARNING 2025-07-04 21:08:00 db_utils.py:141 WandB logging of key "losses_after_forward" was ignored as its type "<class 'torch.Tensor'>" is not handled by this wrapper.
WARNING 2025-07-04 21:08:00 db_utils.py:141 WandB logging of key "losses_after_in_ep_bound" was ignored as its type "<class 'torch.Tensor'>" is not handled by this wrapper.
WARNING 2025-07-04 21:08:00 db_utils.py:141 WandB logging of key "losses_after_rm_padding" was ignored as its type "<class 'torch.Tensor'>" is not handled by this wrapper.
INFO 2025-07-04 21:12:08 ts/train.py:232 step:28K smpl:677K ep:102 epch:50.76 loss:0.011 grdn:0.414 lr:2.7e-06 updt_s:1.237 data_s:0.000
WARNING 2025-07-04 21:12:08 db_utils.py:141 WandB logging of key "losses_after_forward" was ignored as its type "<class 'torch.Tensor'>" is not handled by this wrapper.
WARNING 2025-07-04 21:12:08 db_utils.py:141 WandB logging of key "losses_after_in_ep_bound" was ignored as its type "<class 'torch.Tensor'>" is not handled by this wrapper.
WARNING 2025-07-04 21:12:08 db_utils.py:141 WandB logging of key "losses_after_rm_padding" was ignored as its type "<class 'torch.Tensor'>" is not handled by this wrapper.
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
INFO 2025-07-04 21:16:18 ts/train.py:232 step:28K smpl:682K ep:102 epch:51.12 loss:0.010 grdn:0.397 lr:2.7e-06 updt_s:1.235 data_s:0.013
WARNING 2025-07-04 21:16:18 db_utils.py:141 WandB logging of key "losses_after_forward" was ignored as its type "<class 'torch.Tensor'>" is not handled by this wrapper.
WARNING 2025-07-04 21:16:18 db_utils.py:141 WandB logging of key "losses_after_in_ep_bound" was ignored as its type "<class 'torch.Tensor'>" is not handled by this wrapper.
WARNING 2025-07-04 21:16:18 db_utils.py:141 WandB logging of key "losses_after_rm_padding" was ignored as its type "<class 'torch.Tensor'>" is not handled by this wrapper.
INFO 2025-07-04 21:20:26 ts/train.py:232 step:29K smpl:686K ep:103 epch:51.48 loss:0.010 grdn:0.401 lr:2.6e-06 updt_s:1.237 data_s:0.000
WARNING 2025-07-04 21:20:26 db_utils.py:141 WandB logging of key "losses_after_forward" was ignored as its type "<class 'torch.Tensor'>" is not handled by this wrapper.
WARNING 2025-07-04 21:20:26 db_utils.py:141 WandB logging of key "losses_after_in_ep_bound" was ignored as its type "<class 'torch.Tensor'>" is not handled by this wrapper.
WARNING 2025-07-04 21:20:26 db_utils.py:141 WandB logging of key "losses_after_rm_padding" was ignored as its type "<class 'torch.Tensor'>" is not handled by this wrapper.
INFO 2025-07-04 21:24:33 ts/train.py:232 step:29K smpl:691K ep:104 epch:51.84 loss:0.011 grdn:0.413 lr:2.6e-06 updt_s:1.237 data_s:0.000
WARNING 2025-07-04 21:24:33 db_utils.py:141 WandB logging of key "losses_after_forward" was ignored as its type "<class 'torch.Tensor'>" is not handled by this wrapper.
WARNING 2025-07-04 21:24:33 db_utils.py:141 WandB logging of key "losses_after_in_ep_bound" was ignored as its type "<class 'torch.Tensor'>" is not handled by this wrapper.
WARNING 2025-07-04 21:24:33 db_utils.py:141 WandB logging of key "losses_after_rm_padding" was ignored as its type "<class 'torch.Tensor'>" is not handled by this wrapper.
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
INFO 2025-07-04 21:28:43 ts/train.py:232 step:29K smpl:696K ep:104 epch:52.20 loss:0.010 grdn:0.400 lr:2.6e-06 updt_s:1.235 data_s:0.013
WARNING 2025-07-04 21:28:43 db_utils.py:141 WandB logging of key "losses_after_forward" was ignored as its type "<class 'torch.Tensor'>" is not handled by this wrapper.
WARNING 2025-07-04 21:28:43 db_utils.py:141 WandB logging of key "losses_after_in_ep_bound" was ignored as its type "<class 'torch.Tensor'>" is not handled by this wrapper.
WARNING 2025-07-04 21:28:43 db_utils.py:141 WandB logging of key "losses_after_rm_padding" was ignored as its type "<class 'torch.Tensor'>" is not handled by this wrapper.
INFO 2025-07-04 21:32:51 ts/train.py:232 step:29K smpl:701K ep:105 epch:52.56 loss:0.010 grdn:0.404 lr:2.6e-06 updt_s:1.237 data_s:0.000
WARNING 2025-07-04 21:32:51 db_utils.py:141 WandB logging of key "losses_after_forward" was ignored as its type "<class 'torch.Tensor'>" is not handled by this wrapper.
WARNING 2025-07-04 21:32:51 db_utils.py:141 WandB logging of key "losses_after_in_ep_bound" was ignored as its type "<class 'torch.Tensor'>" is not handled by this wrapper.
WARNING 2025-07-04 21:32:51 db_utils.py:141 WandB logging of key "losses_after_rm_padding" was ignored as its type "<class 'torch.Tensor'>" is not handled by this wrapper.
INFO 2025-07-04 21:36:59 ts/train.py:232 step:29K smpl:706K ep:106 epch:52.92 loss:0.010 grdn:0.397 lr:2.5e-06 updt_s:1.237 data_s:0.000
WARNING 2025-07-04 21:36:59 db_utils.py:141 WandB logging of key "losses_after_forward" was ignored as its type "<class 'torch.Tensor'>" is not handled by this wrapper.
WARNING 2025-07-04 21:36:59 db_utils.py:141 WandB logging of key "losses_after_in_ep_bound" was ignored as its type "<class 'torch.Tensor'>" is not handled by this wrapper.
WARNING 2025-07-04 21:36:59 db_utils.py:141 WandB logging of key "losses_after_rm_padding" was ignored as its type "<class 'torch.Tensor'>" is not handled by this wrapper.
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
INFO 2025-07-04 21:41:08 ts/train.py:232 step:30K smpl:710K ep:107 epch:53.28 loss:0.011 grdn:0.409 lr:2.5e-06 updt_s:1.233 data_s:0.013
WARNING 2025-07-04 21:41:08 db_utils.py:141 WandB logging of key "losses_after_forward" was ignored as its type "<class 'torch.Tensor'>" is not handled by this wrapper.
WARNING 2025-07-04 21:41:08 db_utils.py:141 WandB logging of key "losses_after_in_ep_bound" was ignored as its type "<class 'torch.Tensor'>" is not handled by this wrapper.
WARNING 2025-07-04 21:41:08 db_utils.py:141 WandB logging of key "losses_after_rm_padding" was ignored as its type "<class 'torch.Tensor'>" is not handled by this wrapper.
INFO 2025-07-04 21:45:16 ts/train.py:232 step:30K smpl:715K ep:107 epch:53.64 loss:0.010 grdn:0.401 lr:2.5e-06 updt_s:1.237 data_s:0.000
WARNING 2025-07-04 21:45:16 db_utils.py:141 WandB logging of key "losses_after_forward" was ignored as its type "<class 'torch.Tensor'>" is not handled by this wrapper.
WARNING 2025-07-04 21:45:16 db_utils.py:141 WandB logging of key "losses_after_in_ep_bound" was ignored as its type "<class 'torch.Tensor'>" is not handled by this wrapper.
WARNING 2025-07-04 21:45:16 db_utils.py:141 WandB logging of key "losses_after_rm_padding" was ignored as its type "<class 'torch.Tensor'>" is not handled by this wrapper.
INFO 2025-07-04 21:49:24 ts/train.py:232 step:30K smpl:720K ep:108 epch:54.00 loss:0.011 grdn:0.410 lr:2.5e-06 updt_s:1.238 data_s:0.000
WARNING 2025-07-04 21:49:24 db_utils.py:141 WandB logging of key "losses_after_forward" was ignored as its type "<class 'torch.Tensor'>" is not handled by this wrapper.
WARNING 2025-07-04 21:49:24 db_utils.py:141 WandB logging of key "losses_after_in_ep_bound" was ignored as its type "<class 'torch.Tensor'>" is not handled by this wrapper.
WARNING 2025-07-04 21:49:24 db_utils.py:141 WandB logging of key "losses_after_rm_padding" was ignored as its type "<class 'torch.Tensor'>" is not handled by this wrapper.
INFO 2025-07-04 21:49:24 ts/train.py:241 Checkpoint policy after step 30000
INFO 2025-07-04 21:50:18 ts/train.py:283 End of training
